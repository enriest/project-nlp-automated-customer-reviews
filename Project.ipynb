{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c63d02a",
   "metadata": {},
   "source": [
    "\n",
    "source\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    print(\"NLTK resources ready\")\n",
    "except Exception as e:\n",
    "    print(f\"NLTK resources unavailable or download failed: {e}\")\n",
    "    # Provide lightweight fallbacks so the notebook cell still runs\n",
    "    def word_tokenize(text):\n",
    "        return str(text).split()\n",
    "    stopwords = set()\n",
    "    class _DummyLemmatizer:\n",
    "        def lemmatize(self, w):\n",
    "            return w\n",
    "    WordNetLemmatizer = lambda : _DummyLemmatizer()\n",
    "\n",
    "# Advanced text preprocessing with tokenization, stopword removal, and lemmatization\n",
    "def advanced_preprocess_text(text):\n",
    "    \"\"\"Advanced preprocessing with tokenization, stopword removal, and lemmatization.\"\"\"\n",
    "    # Basic cleaning first\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # TOKENIZATION\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # STOPWORD REMOVAL\n",
    "    try:\n",
    "        sw = set(stopwords.words('english')) if hasattr(stopwords, 'words') else set()\n",
    "    except Exception:\n",
    "        sw = set()\n",
    "    tokens = [w for w in tokens if w not in sw and len(w) > 2]\n",
    "\n",
    "    # LEMMATIZATION\n",
    "    try:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    except Exception:\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"=== ADVANCED TEXT PREPROCESSING ===\")\n",
    "print(\"Applying tokenization, stopword removal, and lemmatization...\")\n",
    "\n",
    "# Auto-detect the text column instead of hardcoding 'reviews.text'\n",
    "candidates = ['reviews.text', 'reviewText', 'text', 'content']\n",
    "text_column = None\n",
    "for c in candidates:\n",
    "    if c in df.columns:\n",
    "        text_column = c\n",
    "        break\n",
    "if text_column is None:\n",
    "    # Fallback: pick the first column that looks like text (contains 'review' or 'text')\n",
    "    for c in df.columns:\n",
    "        if 'review' in c.lower() or 'text' in c.lower():\n",
    "            text_column = c\n",
    "            break\n",
    "# As a last resort, pick the first column\n",
    "if text_column is None:\n",
    "    text_column = df.columns[0]\n",
    "print(f\"Using text column: {text_column}\")\n",
    "\n",
    "# Apply advanced preprocessing to the cleaned text column if present, else to the detected text column\n",
    "source_col = 'cleaned_text' if 'cleaned_text' in df_processed.columns else text_column\n",
    "df_processed['processed_text'] = df_processed[source_col].apply(advanced_preprocess_text)\n",
    "\n",
    "# Remove empty texts after advanced processing\n",
    "df_processed = df_processed[df_processed['processed_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset size after advanced preprocessing: {len(df_processed)}\")\n",
    "\n",
    "# Show examples (if enough rows exist)\n",
    "print(\"\\n=== ADVANCED PREPROCESSING EXAMPLES ===\\\")\n",
    "for i in range(min(3, len(df_processed))):\n",
    "    cleaned = str(df_processed.iloc[i][source_col])[:80] + '...'\n",
    "    processed = df_processed.iloc[i]['processed_text'][:80] + '...'\n",
    "    print(f\"Cleaned:   {cleaned}\")\n",
    "    print(f\"Processed: {processed}\\n\")\n",
    "\n",
    "# Final dataset statistics\n",
    "print(\"=== FINAL PREPROCESSING STATISTICS ===\")\n",
    "avg_length_original = df_processed[text_column].astype(str).str.len().mean() if text_column in df_processed.columns else 0\n",
    "avg_length_processed = df_processed['processed_text'].str.len().mean()\n",
    "avg_words_processed = df_processed['processed_text'].str.split().str.len().mean()\n",
    "\n",
    "print(f\"Average original text length: {avg_length_original:.1f} characters\")\n",
    "print(f\"Average processed text length: {avg_length_processed:.1f} characters\")\n",
    "print(f\"Average words after processing: {avg_words_processed:.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5f7974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.11 (main, Oct 15 2025, 22:06:50) [Clang 17.0.0 (clang-1700.3.19.1)]\n",
      "Python executable: /Users/enriqueestevezalvarez/Documents/Ironhack/Projects/NLP Automated customers/project-nlp-automated-customer-reviews/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b4830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix notebook visualization dependencies\n",
    "import sys\n",
    "import subprocess\n",
    "import nbformat\n",
    "import kaleido\n",
    "\n",
    "# Traditional ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Deep Learning libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Add these new imports for fine-tuning\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "\n",
    "# Dataset loading\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Only add these missing imports in cell 22:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC  # You imported SVC but need LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score  # Individual metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a8799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data resources for text preprocessing\n",
    "try:\n",
    "    # 'punkt' is used for tokenization (splitting text into words/sentences)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    # 'stopwords' provides lists of common words to filter out\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    # 'wordnet' is used for lemmatization (reducing words to their base form)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    # 'omw-1.4' is a multilingual WordNet resource\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"NLTK data downloaded successfully!\")\n",
    "except:\n",
    "    print(\"NLTK data download failed. Please check your internet connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b3037",
   "metadata": {},
   "source": [
    "## STEP 2: Data Collection\n",
    "\n",
    "Loading the Amazon customer reviews dataset from HuggingFace. We'll use a subset to ensure manageable computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f6ea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Amazon US Reviews dataset...\n",
      "âš ï¸  NOTE: The original Amazon US Reviews dataset is no longer available on HuggingFace\n",
      "Trying alternative datasets from HuggingFace...\n",
      "Amazon Reviews Multi not available: Dataset scripts are no longer supported, but found amazon_reviews_multi.py\n",
      "Trying IMDB dataset as HuggingFace alternative...\n",
      "Amazon Reviews Multi not available: Dataset scripts are no longer supported, but found amazon_reviews_multi.py\n",
      "Trying IMDB dataset as HuggingFace alternative...\n",
      "Successfully loaded IMDB dataset as HuggingFace alternative\n",
      "ðŸ“Š Successfully loaded 25000 reviews from HuggingFace dataset\n",
      "ðŸ”„ Also loading local archive data to combine datasets...\n",
      "ðŸ“ Found CSV files in archive: ['Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv', '1429_1.csv', 'Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv']\n",
      "Successfully loaded IMDB dataset as HuggingFace alternative\n",
      "ðŸ“Š Successfully loaded 25000 reviews from HuggingFace dataset\n",
      "ðŸ”„ Also loading local archive data to combine datasets...\n",
      "ðŸ“ Found CSV files in archive: ['Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv', '1429_1.csv', 'Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv']\n",
      "ðŸ“„ Loaded 28332 rows from Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\n",
      "ðŸ“„ Loaded 34660 rows from 1429_1.csv\n",
      "ðŸ“„ Loaded 28332 rows from Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\n",
      "ðŸ“„ Loaded 34660 rows from 1429_1.csv\n",
      "ðŸ“„ Loaded 5000 rows from Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\n",
      "ðŸ“Š Successfully combined all CSV files: 67992 total rows\n",
      "ðŸ“Š Sampled local data to 30000 rows\n",
      "ðŸ”— Combining HuggingFace and local datasets...\n",
      "ðŸŽ¯ COMBINED DATASET: 55000 total reviews\n",
      "   - HuggingFace (IMDB): 25000 reviews\n",
      "   - Local (Amazon): 30000 reviews\n",
      "\n",
      "FINAL DATASET LOADED:\n",
      "Total reviews: 55,000\n",
      "Local_Amazon: 30,000 reviews\n",
      "HuggingFace_IMDB: 25,000 reviews\n",
      "Columns: ['reviews.text', 'reviews.rating', 'data_source', 'id', 'dateAdded', 'dateUpdated', 'name', 'asins', 'brand', 'categories', 'primaryCategories', 'imageURLs', 'keys', 'manufacturer', 'manufacturerNumber', 'reviews.date', 'reviews.dateSeen', 'reviews.didPurchase', 'reviews.doRecommend', 'reviews.id', 'reviews.numHelpful', 'reviews.sourceURLs', 'reviews.title', 'reviews.username', 'sourceURLs', 'reviews.dateAdded', 'reviews.userCity', 'reviews.userProvince']\n",
      "ðŸ“„ Loaded 5000 rows from Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\n",
      "ðŸ“Š Successfully combined all CSV files: 67992 total rows\n",
      "ðŸ“Š Sampled local data to 30000 rows\n",
      "ðŸ”— Combining HuggingFace and local datasets...\n",
      "ðŸŽ¯ COMBINED DATASET: 55000 total reviews\n",
      "   - HuggingFace (IMDB): 25000 reviews\n",
      "   - Local (Amazon): 30000 reviews\n",
      "\n",
      "FINAL DATASET LOADED:\n",
      "Total reviews: 55,000\n",
      "Local_Amazon: 30,000 reviews\n",
      "HuggingFace_IMDB: 25,000 reviews\n",
      "Columns: ['reviews.text', 'reviews.rating', 'data_source', 'id', 'dateAdded', 'dateUpdated', 'name', 'asins', 'brand', 'categories', 'primaryCategories', 'imageURLs', 'keys', 'manufacturer', 'manufacturerNumber', 'reviews.date', 'reviews.dateSeen', 'reviews.didPurchase', 'reviews.doRecommend', 'reviews.id', 'reviews.numHelpful', 'reviews.sourceURLs', 'reviews.title', 'reviews.username', 'sourceURLs', 'reviews.dateAdded', 'reviews.userCity', 'reviews.userProvince']\n"
     ]
    }
   ],
   "source": [
    "# Load Amazon US Reviews dataset from HuggingFace\n",
    "# We'll use the \"Electronics\" category for manageable size\n",
    "print(\"Loading Amazon US Reviews dataset...\")\n",
    "\n",
    "try:\n",
    "    print(\"âš ï¸  NOTE: The original Amazon US Reviews dataset is no longer available on HuggingFace\")\n",
    "    print(\"Trying alternative datasets from HuggingFace...\")\n",
    "    \n",
    "    # Try the newer Amazon reviews dataset first\n",
    "    try:\n",
    "        dataset = load_dataset(\"amazon_reviews_multi\", \"en\", split=\"train\")\n",
    "        df = dataset.to_pandas()\n",
    "        # Rename columns to match expected format\n",
    "        df = df.rename(columns={\n",
    "            'review_body': 'reviews.text',\n",
    "            'stars': 'reviews.rating'\n",
    "        })\n",
    "        print(\"Successfully loaded Amazon Reviews Multi dataset\")\n",
    "    except Exception as e1:\n",
    "        print(f\"Amazon Reviews Multi not available: {e1}\")\n",
    "        print(\"Trying IMDB dataset as HuggingFace alternative...\")\n",
    "        \n",
    "        try:\n",
    "            # Alternative: Use IMDB dataset and adapt it\n",
    "            dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "            df = dataset.to_pandas()\n",
    "            # Convert IMDB labels (0=negative, 1=positive) to ratings (1-5 scale)\n",
    "            df['reviews.rating'] = df['label'].map({0: 2, 1: 5})  # Map to low and high ratings\n",
    "            df = df.rename(columns={'text': 'reviews.text'})\n",
    "            df = df.drop('label', axis=1)\n",
    "            print(\"Successfully loaded IMDB dataset as HuggingFace alternative\")\n",
    "        except Exception as e2:\n",
    "            print(f\"IMDB dataset also failed: {e2}\")\n",
    "            raise Exception(\"All HuggingFace datasets failed\")\n",
    "    \n",
    "    # Take a sample to manage computational resources (adjust size based on your needs)\n",
    "    sample_size = min(30000, len(df))  # Use up to 30k reviews from HuggingFace\n",
    "    df_huggingface = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"ðŸ“Š Successfully loaded {len(df_huggingface)} reviews from HuggingFace dataset\")\n",
    "    \n",
    "    # Now also load local archive data to combine with HuggingFace data\n",
    "    print(\"ðŸ”„ Also loading local archive data to combine datasets...\")\n",
    "    df_local = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading HuggingFace dataset: {e}\")\n",
    "    print(\"Loading dataset from local archive folder only...\")\n",
    "    df_huggingface = None\n",
    "    \n",
    "# Load dataset from archive folder (for combination or as fallback)\n",
    "try:\n",
    "    import os\n",
    "    archive_path = \"archive\"\n",
    "    \n",
    "    # Look for CSV files in the archive folder\n",
    "    if os.path.exists(archive_path):\n",
    "        csv_files = [f for f in os.listdir(archive_path) if f.endswith('.csv')]\n",
    "        print(f\"ðŸ“ Found CSV files in archive: {csv_files}\")\n",
    "        \n",
    "        if csv_files:\n",
    "            # Load and combine all CSV files\n",
    "            dataframes = []\n",
    "            for csv_file in csv_files:\n",
    "                file_path = os.path.join(archive_path, csv_file)\n",
    "                try:\n",
    "                    temp_df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    temp_df = pd.read_csv(file_path, encoding='latin-1')\n",
    "                print(f\"ðŸ“„ Loaded {len(temp_df)} rows from {csv_file}\")\n",
    "                dataframes.append(temp_df)\n",
    "            \n",
    "            # Combine all CSV dataframes\n",
    "            df_local = pd.concat(dataframes, ignore_index=True)\n",
    "            print(f\"ðŸ“Š Successfully combined all CSV files: {len(df_local)} total rows\")\n",
    "            \n",
    "            # Take a sample from local data (leave room for HuggingFace data)\n",
    "            local_sample_size = 30000 if df_huggingface is not None else 50000\n",
    "            if len(df_local) > local_sample_size:\n",
    "                df_local = df_local.sample(n=local_sample_size, random_state=42).reset_index(drop=True)\n",
    "                print(f\"ðŸ“Š Sampled local data to {len(df_local)} rows\")\n",
    "            \n",
    "            # Combine HuggingFace and local data if both available\n",
    "            if df_huggingface is not None:\n",
    "                print(\"ðŸ”— Combining HuggingFace and local datasets...\")\n",
    "                \n",
    "                # Standardize column names for both datasets\n",
    "                # HuggingFace data already has 'reviews.text' and 'reviews.rating'\n",
    "                # Local data might have different column names, so map them\n",
    "                if 'reviews.text' not in df_local.columns:\n",
    "                    # Find text column in local data\n",
    "                    text_cols = ['reviews.text', 'review_body', 'review_text', 'text', 'body']\n",
    "                    for col in text_cols:\n",
    "                        if col in df_local.columns:\n",
    "                            df_local = df_local.rename(columns={col: 'reviews.text'})\n",
    "                            break\n",
    "                \n",
    "                if 'reviews.rating' not in df_local.columns:\n",
    "                    # Find rating column in local data  \n",
    "                    rating_cols = ['reviews.rating', 'star_rating', 'rating', 'stars']\n",
    "                    for col in rating_cols:\n",
    "                        if col in df_local.columns:\n",
    "                            df_local = df_local.rename(columns={col: 'reviews.rating'})\n",
    "                            break\n",
    "                \n",
    "                # Add source identifier\n",
    "                df_huggingface['data_source'] = 'HuggingFace_IMDB'\n",
    "                df_local['data_source'] = 'Local_Amazon'\n",
    "                \n",
    "                # Combine datasets\n",
    "                df = pd.concat([df_huggingface, df_local], ignore_index=True)\n",
    "                print(f\"ðŸŽ¯ COMBINED DATASET: {len(df)} total reviews\")\n",
    "                print(f\"   - HuggingFace (IMDB): {len(df_huggingface)} reviews\")\n",
    "                print(f\"   - Local (Amazon): {len(df_local)} reviews\")\n",
    "                \n",
    "            else:\n",
    "                # Only local data available\n",
    "                df = df_local\n",
    "                df['data_source'] = 'Local_Amazon'\n",
    "                print(f\"ðŸ“Š Using local dataset only: {len(df)} reviews\")\n",
    "                \n",
    "        else:\n",
    "            raise FileNotFoundError(\"No CSV files found in archive folder\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Archive folder not found\")\n",
    "        \n",
    "except Exception as e2:\n",
    "    print(f\"Error loading from archive: {e2}\")\n",
    "    \n",
    "    # If we have HuggingFace data but no local data, use HuggingFace only\n",
    "    if 'df_huggingface' in locals() and df_huggingface is not None:\n",
    "        df = df_huggingface\n",
    "        df['data_source'] = 'HuggingFace_IMDB'\n",
    "        print(f\"Using HuggingFace dataset only: {len(df)} reviews\")\n",
    "    else:\n",
    "        # Neither source worked, use dummy data\n",
    "        print(\"Using dummy data for demonstration. Please check your archive folder path.\")\n",
    "        df = pd.DataFrame({\n",
    "            'reviews.text': ['This product is amazing!', 'Poor quality, disappointed', 'Average product, okay'],\n",
    "            'reviews.rating': [5, 2, 4],\n",
    "            'data_source': ['Dummy', 'Dummy', 'Dummy']\n",
    "        })\n",
    "        print(\"ðŸ“Š Using dummy data for demonstration.\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nFINAL DATASET LOADED:\")\n",
    "print(f\"Total reviews: {len(df):,}\")\n",
    "if 'data_source' in df.columns:\n",
    "    source_counts = df['data_source'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"{source}: {count:,} reviews\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e1565",
   "metadata": {},
   "source": [
    "## STEP 3: Data Understanding\n",
    "\n",
    "Exploring the dataset structure, checking columns, and examining data distribution and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9489518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET OVERVIEW ===\n",
      "Dataset shape: (55000, 28)\n",
      "Columns: ['reviews.text', 'reviews.rating', 'data_source', 'id', 'dateAdded', 'dateUpdated', 'name', 'asins', 'brand', 'categories', 'primaryCategories', 'imageURLs', 'keys', 'manufacturer', 'manufacturerNumber', 'reviews.date', 'reviews.dateSeen', 'reviews.didPurchase', 'reviews.doRecommend', 'reviews.id', 'reviews.numHelpful', 'reviews.sourceURLs', 'reviews.title', 'reviews.username', 'sourceURLs', 'reviews.dateAdded', 'reviews.userCity', 'reviews.userProvince']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFINE-TUNING RESULTS SUMMARY\\n\\n\\n# Fine-tuning is now handled in fine-tuning.py\\n# To run fine-tuning, import and call run_fine_tuning from fine-tuning.py\\n# Example:\\n# from fine_tuning import run_fine_tuning\\n# fine_tuned_results = run_fine_tuning(transformer_data, models_to_finetune, device)\\n# If not running fine-tuning, ensure fine_tuned_results is an empty dict:\\nfine_tuned_results = {}  # Guard for evaluation cells\\n        models_to_finetune = {\\n            \\'DistilBERT\\': \\'./offline_models/models--distilbert-base-uncased\\',\\n            \\'RoBERTa\\': \\'./offline_models/models--roberta-base\\'\\n        }\\n        print(f\"\\nâœ… Using locally cached models from: ./offline_models/\")\\n\\n        print(f\"\\nðŸ“‹ Models selected for fine-tuning:\")\\n        for model_name, model_path in models_to_finetune.items():\\n            print(f\"   â€¢ {model_name}: {model_path}\")\\n\\n        print(f\"\\nâœ… Setup complete! Ready for fine-tuning.\")\\n        print(f\"  {col}: {sample_val}...\")\\n        \\n        I CANNOT ASUME FINE-TUNING WOULD RUN SUCCESSFULLY'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic dataset information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "\"\"\"\n",
    "FINE-TUNING RESULTS SUMMARY\n",
    "\n",
    "\n",
    "# Fine-tuning is now handled in fine-tuning.py\n",
    "# To run fine-tuning, import and call run_fine_tuning from fine-tuning.py\n",
    "# Example:\n",
    "# from fine_tuning import run_fine_tuning\n",
    "# fine_tuned_results = run_fine_tuning(transformer_data, models_to_finetune, device)\n",
    "# If not running fine-tuning, ensure fine_tuned_results is an empty dict:\n",
    "fine_tuned_results = {}  # Guard for evaluation cells\n",
    "        models_to_finetune = {\n",
    "            'DistilBERT': './offline_models/models--distilbert-base-uncased',\n",
    "            'RoBERTa': './offline_models/models--roberta-base'\n",
    "        }\n",
    "        print(f\"\\nâœ… Using locally cached models from: ./offline_models/\")\n",
    "\n",
    "        print(f\"\\nðŸ“‹ Models selected for fine-tuning:\")\n",
    "        for model_name, model_path in models_to_finetune.items():\n",
    "            print(f\"   â€¢ {model_name}: {model_path}\")\n",
    "\n",
    "        print(f\"\\nâœ… Setup complete! Ready for fine-tuning.\")\n",
    "        print(f\"  {col}: {sample_val}...\")\n",
    "        \n",
    "        I CANNOT ASUME FINE-TUNING WOULD RUN SUCCESSFULLY\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df7489",
   "metadata": {},
   "source": [
    "## STEP 4: Target Variable Creation\n",
    "\n",
    "Transforming ratings into sentiment labels according to the specified logic:\n",
    "- Scores 1, 2, 3 â†’ \"Negative\"\n",
    "- Score 4 â†’ \"Neutral\" \n",
    "- Score 5 â†’ \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d31dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SENTIMENT TRANSFORMATION RESULTS ===\n",
      "Using rating column: 'reviews.rating'\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "Positive    33252\n",
      "Negative    14958\n",
      "Neutral      6776\n",
      "Unknown        14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment percentages:\n",
      "Positive: 60.46%\n",
      "Negative: 27.2%\n",
      "Neutral: 12.32%\n",
      "Unknown: 0.03%\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "hovertemplate": "label=%{label}<br>value=%{value}<extra></extra>",
         "labels": [
          "Positive",
          "Negative",
          "Neutral",
          "Unknown"
         ],
         "legendgroup": "",
         "name": "",
         "showlegend": true,
         "type": "pie",
         "values": {
          "bdata": "5IEAAG46AAB4GgAADgAAAA==",
          "dtype": "i4"
         }
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Sentiment Distribution After Transformation"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MAPPING VERIFICATION ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>12982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>6776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>33252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviews.rating sentiment  count\n",
       "0             1.0  Negative    673\n",
       "1             2.0  Negative  12982\n",
       "2             3.0  Negative   1303\n",
       "3             4.0   Neutral   6776\n",
       "4             5.0  Positive  33252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create sentiment labels based on star ratings\n",
    "def create_sentiment_labels(rating):\n",
    "    \"\"\"\n",
    "    Transform numerical ratings to sentiment labels\n",
    "    1, 2, 3 -> Negative\n",
    "    4 -> Neutral\n",
    "    5 -> Positive\n",
    "    \"\"\"\n",
    "    if rating in [1, 2, 3]:\n",
    "        return 'Negative'\n",
    "    elif rating == 4:\n",
    "        return 'Neutral'\n",
    "    elif rating == 5:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Unknown'  # For any unexpected values\n",
    "\n",
    "# Apply the transformation\n",
    "rating_column = 'reviews.rating'\n",
    "\n",
    "if rating_column in df.columns:\n",
    "    df['sentiment'] = df[rating_column].apply(create_sentiment_labels)\n",
    "    \n",
    "    print(\"=== SENTIMENT TRANSFORMATION RESULTS ===\")\n",
    "    print(f\"Using rating column: '{rating_column}'\")\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    print(\"Sentiment distribution:\")\n",
    "    print(sentiment_counts)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    sentiment_percentages = (sentiment_counts / len(df) * 100).round(2)\n",
    "    print(\"\\nSentiment percentages:\")\n",
    "    for sentiment, percentage in sentiment_percentages.items():\n",
    "        print(f\"{sentiment}: {percentage}%\")\n",
    "    \n",
    "    # Visualize the new sentiment distribution\n",
    "    try:\n",
    "        fig = px.pie(values=sentiment_counts.values, names=sentiment_counts.index, \n",
    "                     title='Sentiment Distribution After Transformation')\n",
    "        fig.show()\n",
    "    except Exception as plot_error:\n",
    "        print(f\"Plotly visualization error: {plot_error}\")\n",
    "        print(\"Using matplotlib as fallback:\")\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "        plt.title('Sentiment Distribution After Transformation')\n",
    "        plt.show()\n",
    "    \n",
    "    # Show the mapping visually\n",
    "    mapping_df = df.groupby([rating_column, 'sentiment']).size().reset_index(name='count')\n",
    "    print(f\"\\n=== MAPPING VERIFICATION ===\")\n",
    "    display(mapping_df)\n",
    "    \n",
    "else:\n",
    "    print(f\"Rating column '{rating_column}' not found. Please check your dataset structure.\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Try to find alternative rating columns as fallback\n",
    "    possible_alternatives = ['rating', 'star_rating', 'score', 'stars', 'overall']\n",
    "    found_alternative = None\n",
    "    for alt_col in possible_alternatives:\n",
    "        if alt_col in df.columns:\n",
    "            found_alternative = alt_col\n",
    "            break\n",
    "    \n",
    "    if found_alternative:\n",
    "        print(f\"Found alternative rating column: '{found_alternative}'. Using this instead.\")\n",
    "        df['sentiment'] = df[found_alternative].apply(create_sentiment_labels)\n",
    "        rating_column = found_alternative  # Update for later use\n",
    "    else:\n",
    "        print(\"No suitable rating column found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ff1587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL DATASET PREPARATION ===\n",
      "Original dataset size: 55000\n",
      "Clean dataset size: 54617\n",
      "Removed 383 rows\n",
      "\n",
      "=== FINAL DATASET SUMMARY ===\n",
      "Total reviews: 54617\n",
      "Text column: 'reviews.text'\n",
      "Target column: 'sentiment'\n",
      "Sentiment distribution:\n",
      "Original dataset size: 55000\n",
      "Clean dataset size: 54617\n",
      "Removed 383 rows\n",
      "\n",
      "=== FINAL DATASET SUMMARY ===\n",
      "Total reviews: 54617\n",
      "Text column: 'reviews.text'\n",
      "Target column: 'sentiment'\n",
      "Sentiment distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    32953\n",
       "Negative    14941\n",
       "Neutral      6723\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean and prepare the final dataset\n",
    "print(\"=== FINAL DATASET PREPARATION ===\")\n",
    "\n",
    "# Define the text column name\n",
    "text_column = 'reviews.text'\n",
    "\n",
    "# Remove rows with missing essential data\n",
    "if text_column and 'sentiment' in df.columns:\n",
    "    # Keep only rows with valid text and sentiment\n",
    "    df_clean = df.dropna(subset=[text_column, 'sentiment']).copy()\n",
    "    \n",
    "    # Remove very short reviews (less than 10 characters)\n",
    "    df_clean = df_clean[df_clean[text_column].str.len() >= 10].copy()\n",
    "    \n",
    "    # Remove 'Unknown' sentiment labels if any\n",
    "    df_clean = df_clean[df_clean['sentiment'] != 'Unknown'].copy()\n",
    "    \n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(f\"Clean dataset size: {len(df_clean)}\")\n",
    "    print(f\"Removed {len(df) - len(df_clean)} rows\")\n",
    "    \n",
    "    # Update the main dataframe\n",
    "    df = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n=== FINAL DATASET SUMMARY ===\")\n",
    "    print(f\"Total reviews: {len(df)}\")\n",
    "    print(f\"Text column: '{text_column}'\")\n",
    "    print(f\"Target column: 'sentiment'\")\n",
    "    print(f\"Sentiment distribution:\")\n",
    "    sentiment_final = df['sentiment'].value_counts()\n",
    "    display(sentiment_final)\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot proceed without valid text column and sentiment labels.\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72222ef",
   "metadata": {},
   "source": [
    "Imbalance handling strategy (3-class: ~60% Positive, 27% Negative, 12% Neutral)\n",
    "\n",
    "What we changed:\n",
    "- Class weights: Enabled class_weight='balanced' for Logistic Regression and Linear SVM; balanced_subsample for Random Forest; balanced for Extra Trees. This upweights under-represented classes during training without duplicating data.\n",
    "- Mild over-sampling (train only): Upsampled Neutral to match Negative (did not fully balance to Positive) with RandomOverSampler. Test set remains untouched to keep evaluation honest.\n",
    "- XGBoost weighting: Passed per-sample weights from class weights during fit (multi-class).\n",
    "- Metrics: Added macro-F1 to monitor minority-class performance; still report weighted metrics and per-class precision/recall/F1.\n",
    "\n",
    "Why this setup:\n",
    "- Class weights shift the decision boundary toward minority classes with minimal risk of overfitting.\n",
    "- Mild over-sampling gives Neutral more representation without over-amplifying noise (we avoid fully balancing to Positive).\n",
    "- Macro-F1 counters accuracy inflation from the dominant Positive class and surfaces Neutral/Negative recall.\n",
    "- This approach is robust for real-world imbalanced data, avoids overfitting, and keeps test evaluation honest.\n",
    "- By not fully balancing to Positive, we avoid introducing too much synthetic data/noise for the majority class.\n",
    "- Macro-F1 and per-class metrics help us track Neutral/Negative performance, not just overall accuracy.\n",
    "\n",
    "Consequences:\n",
    "- Models are less likely to ignore Neutral/Negative, but may still have lower recall for Neutral if the signal is weak.\n",
    "- If Neutral recall is still low, consider threshold tuning, stronger regularization, or slightly more Neutral upsampling.\n",
    "- For transformers, we use a class-weighted CrossEntropyLoss in fine-tuning (WeightedTrainer) instead of resampling.\n",
    "- Always evaluate with macro-F1 and per-class recall, not just accuracy.\n",
    "- This setup is easy to maintain and extend if class distributions shift in future data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eeb955",
   "metadata": {},
   "source": [
    "## STEP 5: Traditional NLP & ML Approach\n",
    "\n",
    "Implementing traditional machine learning approach with text preprocessing, vectorization, and multiple ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd898208",
   "metadata": {},
   "source": [
    "### 5.1 Data Preprocessing for Traditional ML\n",
    "\n",
    "Text cleaning, tokenization, lemmatization, and vectorization for traditional machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f15d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEXT PREPROCESSING ===\n",
      "Applying text cleaning and preprocessing...\n",
      "Dataset size after text cleaning: 54616\n",
      "Removed 1 rows with empty text after cleaning\n",
      "\n",
      "=== PREPROCESSING EXAMPLES ===\n",
      "Original: Dumb is as dumb does, in this thoroughly uninteresting, supposed black comedy. Essentially what star...\n",
      "Cleaned:  dumb is as dumb does in this thoroughly uninteresting supposed black comedy essentially what starts ...\n",
      "\n",
      "Original: I dug out from my garage some old musicals and this is another one of my favorites. It was written b...\n",
      "Cleaned:  i dug out from my garage some old musicals and this is another one of my favorites it was written by...\n",
      "\n",
      "Original: After watching this movie I was honestly disappointed - not because of the actors, story or directin...\n",
      "Cleaned:  after watching this movie i was honestly disappointed not because of the actors story or directing i...\n",
      "\n",
      "Dataset size after text cleaning: 54616\n",
      "Removed 1 rows with empty text after cleaning\n",
      "\n",
      "=== PREPROCESSING EXAMPLES ===\n",
      "Original: Dumb is as dumb does, in this thoroughly uninteresting, supposed black comedy. Essentially what star...\n",
      "Cleaned:  dumb is as dumb does in this thoroughly uninteresting supposed black comedy essentially what starts ...\n",
      "\n",
      "Original: I dug out from my garage some old musicals and this is another one of my favorites. It was written b...\n",
      "Cleaned:  i dug out from my garage some old musicals and this is another one of my favorites it was written by...\n",
      "\n",
      "Original: After watching this movie I was honestly disappointed - not because of the actors, story or directin...\n",
      "Cleaned:  after watching this movie i was honestly disappointed not because of the actors story or directing i...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data for traditional ML\n",
    "    \"\"\"\n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters and digits, keep only letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text preprocessing\n",
    "print(\"=== TEXT PREPROCESSING ===\")\n",
    "print(\"Applying text cleaning and preprocessing...\")\n",
    "\n",
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Apply preprocessing to text column\n",
    "df_processed['cleaned_text'] = df_processed[text_column].apply(preprocess_text)\n",
    "\n",
    "# Remove empty texts after cleaning\n",
    "df_processed = df_processed[df_processed['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset size after text cleaning: {len(df_processed)}\")\n",
    "print(f\"Removed {len(df) - len(df_processed)} rows with empty text after cleaning\")\n",
    "\n",
    "# Show examples of cleaned text\n",
    "print(\"\\n=== PREPROCESSING EXAMPLES ===\")\n",
    "for i in range(3):\n",
    "    original = str(df_processed.iloc[i][text_column])[:100] + \"...\"\n",
    "    cleaned = df_processed.iloc[i]['cleaned_text'][:100] + \"...\"\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Cleaned:  {cleaned}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87da6f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded punkt_tab tokenizer\n",
      "=== ADVANCED TEXT PREPROCESSING ===\n",
      "Applying tokenization, stopword removal, and lemmatization...\n",
      "Using text column: reviews.text\n",
      "Dataset size after advanced preprocessing: 54615\n",
      "\n",
      "=== ADVANCED PREPROCESSING EXAMPLES ===\n",
      "Cleaned:   dumb is as dumb does in this thoroughly uninteresting supposed black comedy esse...\n",
      "Processed: dumb dumb thoroughly uninteresting supposed black comedy essentially start chris...\n",
      "\n",
      "Cleaned:   i dug out from my garage some old musicals and this is another one of my favorit...\n",
      "Processed: dug garage old musical another one favorite written jay alan lerner directed vin...\n",
      "\n",
      "Cleaned:   after watching this movie i was honestly disappointed not because of the actors ...\n",
      "Processed: watching movie honestly disappointed actor story directing disappointed film adv...\n",
      "\n",
      "=== FINAL PREPROCESSING STATISTICS ===\n",
      "Average original text length: 688.7 characters\n",
      "Average processed text length: 428.3 characters\n",
      "Average words after processing: 62.2 words\n",
      "Dataset size after advanced preprocessing: 54615\n",
      "\n",
      "=== ADVANCED PREPROCESSING EXAMPLES ===\n",
      "Cleaned:   dumb is as dumb does in this thoroughly uninteresting supposed black comedy esse...\n",
      "Processed: dumb dumb thoroughly uninteresting supposed black comedy essentially start chris...\n",
      "\n",
      "Cleaned:   i dug out from my garage some old musicals and this is another one of my favorit...\n",
      "Processed: dug garage old musical another one favorite written jay alan lerner directed vin...\n",
      "\n",
      "Cleaned:   after watching this movie i was honestly disappointed not because of the actors ...\n",
      "Processed: watching movie honestly disappointed actor story directing disappointed film adv...\n",
      "\n",
      "=== FINAL PREPROCESSING STATISTICS ===\n",
      "Average original text length: 688.7 characters\n",
      "Average processed text length: 428.3 characters\n",
      "Average words after processing: 62.2 words\n"
     ]
    }
   ],
   "source": [
    "# Download additional NLTK resources needed for advanced preprocessing\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"Downloaded punkt_tab tokenizer\")\n",
    "except:\n",
    "    print(\"punkt_tab download failed, trying alternative...\")\n",
    "\n",
    "# Advanced text preprocessing with NLTK\n",
    "def advanced_preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Advanced preprocessing with tokenization, stopword removal, and lemmatization\n",
    "    \n",
    "    This function performs three key NLP preprocessing steps:\n",
    "    \n",
    "    1. TOKENIZATION: Breaking text into individual words/tokens\n",
    "       - Purpose: Converts sentences into lists of words for analysis\n",
    "       - Example: \"I love this product!\" â†’ [\"I\", \"love\", \"this\", \"product\"]\n",
    "       - Why needed: ML algorithms work with individual features, not sentences\n",
    "    \n",
    "    2. STOPWORD REMOVAL: Filtering out common, non-informative words\n",
    "       - Purpose: Remove words like \"the\", \"and\", \"is\" that don't carry sentiment\n",
    "       - Example: [\"I\", \"love\", \"this\", \"product\"] â†’ [\"love\", \"product\"]\n",
    "       - Why needed: Focuses on meaningful words, reduces noise and dimensionality\n",
    "    \n",
    "    3. LEMMATIZATION: Converting words to their root/base form\n",
    "       - Purpose: Groups related word forms together (runningâ†’run, betterâ†’good)\n",
    "       - Example: [\"running\", \"runs\", \"ran\"] â†’ [\"run\", \"run\", \"run\"]\n",
    "       - Why needed: Reduces vocabulary size, improves feature consistency\n",
    "    \n",
    "    4. VECTORIZATION (happens later): Converting text to numerical vectors\n",
    "       - Purpose: Transform words into numbers that ML algorithms can process\n",
    "       - Methods: Count (word frequency) or TF-IDF (importance weighting)\n",
    "       - Why needed: ML models require numerical input, not text\n",
    "    \"\"\"\n",
    "    # Basic cleaning\n",
    "    text = preprocess_text(text)\n",
    "    \n",
    "    try:\n",
    "        # TOKENIZATION: Split text into individual words/tokens\n",
    "        # Example: \"great product quality\" â†’ [\"great\", \"product\", \"quality\"]\n",
    "        tokens = word_tokenize(text)\n",
    "    except LookupError:\n",
    "        # Fallback to simple split if NLTK tokenizer fails\n",
    "        tokens = text.split()\n",
    "    \n",
    "    # STOPWORD REMOVAL: Filter out common, non-informative words\n",
    "    # Removes: \"the\", \"and\", \"is\", \"in\", \"to\", \"of\", etc.\n",
    "    # Keeps: meaningful words that carry sentiment or content information\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    except LookupError:\n",
    "        # If stopwords not available, just filter by length\n",
    "        tokens = [word for word in tokens if len(word) > 2]\n",
    "    \n",
    "    # LEMMATIZATION: Convert words to their base/root form\n",
    "    # Examples: \"running\" â†’ \"run\", \"better\" â†’ \"good\", \"cats\" â†’ \"cat\"\n",
    "    # This groups similar word forms together for better feature consistency\n",
    "    try:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    except LookupError:\n",
    "        # If lemmatizer not available, just lowercase\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"=== ADVANCED TEXT PREPROCESSING ===\")\n",
    "print(\"Applying tokenization, stopword removal, and lemmatization...\")\n",
    "\n",
    "# Fix text column identification - use the correct column name\n",
    "text_column = 'reviews.text'\n",
    "print(f\"Using text column: {text_column}\")\n",
    "\n",
    "# Apply advanced preprocessing\n",
    "# This will transform: \"I really love this amazing product!\" \n",
    "# Into: \"really love amazing product\" (tokenized, stopwords removed, lemmatized)\n",
    "df_processed['processed_text'] = df_processed['cleaned_text'].apply(advanced_preprocess_text)\n",
    "\n",
    "# Remove empty texts after advanced processing\n",
    "df_processed = df_processed[df_processed['processed_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset size after advanced preprocessing: {len(df_processed)}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n=== ADVANCED PREPROCESSING EXAMPLES ===\")\n",
    "for i in range(3):\n",
    "    cleaned = df_processed.iloc[i]['cleaned_text'][:80] + \"...\"\n",
    "    processed = df_processed.iloc[i]['processed_text'][:80] + \"...\"\n",
    "    print(f\"Cleaned:   {cleaned}\")\n",
    "    print(f\"Processed: {processed}\\n\")\n",
    "\n",
    "# Final dataset statistics\n",
    "print(\"=== FINAL PREPROCESSING STATISTICS ===\")\n",
    "avg_length_original = df_processed[text_column].astype(str).str.len().mean()\n",
    "avg_length_processed = df_processed['processed_text'].str.len().mean()\n",
    "avg_words_processed = df_processed['processed_text'].str.split().str.len().mean()\n",
    "\n",
    "print(f\"Average original text length: {avg_length_original:.1f} characters\")\n",
    "print(f\"Average processed text length: {avg_length_processed:.1f} characters\")\n",
    "print(f\"Average words after processing: {avg_words_processed:.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0daa84c",
   "metadata": {},
   "source": [
    "### 5.2 Vectorization\n",
    "\n",
    "Converting text data into numerical vectors using CountVectorizer and TF-IDF Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67cfea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VECTORIZATION SETUP ===\n",
      "Feature shape: (54615,)\n",
      "Target distribution:\n",
      "sentiment\n",
      "Positive    32951\n",
      "Negative    14941\n",
      "Neutral      6723\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train set size: 43692\n",
      "Test set size: 10923\n",
      "Train set distribution:\n",
      "sentiment\n",
      "Positive    26361\n",
      "Negative    11953\n",
      "Neutral      5378\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== COUNT VECTORIZATION ===\n",
      "\n",
      "Train set size: 43692\n",
      "Test set size: 10923\n",
      "Train set distribution:\n",
      "sentiment\n",
      "Positive    26361\n",
      "Negative    11953\n",
      "Neutral      5378\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== COUNT VECTORIZATION ===\n",
      "Count vectorizer vocabulary size: 5000\n",
      "Count matrix shape - Train: (43692, 5000), Test: (10923, 5000)\n",
      "\n",
      "=== TF-IDF VECTORIZATION ===\n",
      "Count vectorizer vocabulary size: 5000\n",
      "Count matrix shape - Train: (43692, 5000), Test: (10923, 5000)\n",
      "\n",
      "=== TF-IDF VECTORIZATION ===\n",
      "TF-IDF vectorizer vocabulary size: 5000\n",
      "TF-IDF matrix shape - Train: (43692, 5000), Test: (10923, 5000)\n",
      "\n",
      "=== TOP FEATURES ===\n",
      "Top 20 features by CountVectorizer:\n",
      "['aaa' 'abandoned' 'abc' 'ability' 'able' 'able get' 'absence' 'absolute'\n",
      " 'absolutely' 'absolutely love' 'absolutely nothing' 'absurd' 'abuse'\n",
      " 'academy' 'academy award' 'accent' 'accept' 'acceptable' 'accepted'\n",
      " 'access']\n",
      "TF-IDF vectorizer vocabulary size: 5000\n",
      "TF-IDF matrix shape - Train: (43692, 5000), Test: (10923, 5000)\n",
      "\n",
      "=== TOP FEATURES ===\n",
      "Top 20 features by CountVectorizer:\n",
      "['aaa' 'abandoned' 'abc' 'ability' 'able' 'able get' 'absence' 'absolute'\n",
      " 'absolutely' 'absolutely love' 'absolutely nothing' 'absurd' 'abuse'\n",
      " 'academy' 'academy award' 'accent' 'accept' 'acceptable' 'accepted'\n",
      " 'access']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nVECTORIZATION COMPARISON:\\n- Count Vectorizer: Simple word frequency counting\\n  * Pros: Simple, fast, good baseline\\n  * Cons: Doesn't consider word importance across documents\\n  \\n- TF-IDF Vectorizer: Importance-weighted word frequency\\n  * Pros: Considers word rarity, better for distinguishing documents\\n  * Cons: Slightly more complex, can be sensitive to document collection\\n\\nNEXT STEPS:\\nBoth vectorized datasets (X_train_count, X_train_tfidf) will be used to train\\ndifferent machine learning models to compare which vectorization method works\\nbetter for sentiment analysis on this specific dataset.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "VECTORIZATION AND DATA PREPARATION FOR MACHINE LEARNING\n",
    "\n",
    "This cell converts preprocessed text data into numerical vectors that machine learning algorithms can understand.\n",
    "It prepares the data in two different vectorization formats for model comparison.\n",
    "\n",
    "KEY PURPOSES:\n",
    "1. Data Splitting: Divide dataset into training and testing sets\n",
    "2. Count Vectorization: Convert text to word frequency vectors\n",
    "3. TF-IDF Vectorization: Convert text to importance-weighted vectors\n",
    "4. Feature Engineering: Create numerical representations of text data\n",
    "\n",
    "WHY THIS STEP IS ESSENTIAL:\n",
    "- Machine learning algorithms only work with numbers, not text\n",
    "- Vectorization transforms words into mathematical features\n",
    "- Different vectorization methods capture different aspects of text meaning\n",
    "- Proper train/test split ensures unbiased model evaluation\n",
    "\"\"\"\n",
    "\n",
    "# Prepare data for vectorization\n",
    "print(\"=== VECTORIZATION SETUP ===\")\n",
    "\n",
    "# STEP 1: Prepare features (X) and target variable (y)\n",
    "# Features: The processed text that will be converted to numbers\n",
    "# Target: The sentiment labels we want to predict\n",
    "X = df_processed['processed_text']  # Input features (text)\n",
    "y = df_processed['sentiment']       # Target variable (Negative/Neutral/Positive)\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# STEP 2: Train-Test Split\n",
    "# Purpose: Separate data for training models and testing their performance\n",
    "# - 80% for training (model learns from this)\n",
    "# - 20% for testing (unbiased evaluation)\n",
    "# - stratify=y ensures balanced sentiment distribution in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Train set distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# STEP 3: COUNT VECTORIZATION\n",
    "# Purpose: Convert text to numerical vectors based on word frequency\n",
    "# How it works: Each word becomes a feature, value = how many times it appears\n",
    "# Example: \"love product\" â†’ [0, 1, 0, 1, 0] (if vocabulary is [bad, love, hate, product, terrible])\n",
    "print(\"\\n=== COUNT VECTORIZATION ===\")\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=5000,  # Limit vocabulary to top 5000 most frequent words\n",
    "    ngram_range=(1, 2),  # Include single words (unigrams) and word pairs (bigrams)\n",
    "    min_df=2,  # Ignore words that appear in less than 2 documents (remove rare words)\n",
    "    max_df=0.8  # Ignore words that appear in more than 80% of documents (remove too common words)\n",
    ")\n",
    "\n",
    "# Transform training data (fit learns vocabulary, transform converts to numbers)\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "# Transform test data (only transform, don't learn new vocabulary)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Count vectorizer vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
    "print(f\"Count matrix shape - Train: {X_train_count.shape}, Test: {X_test_count.shape}\")\n",
    "\n",
    "# STEP 4: TF-IDF VECTORIZATION\n",
    "# Purpose: Convert text to numerical vectors based on word importance\n",
    "# How it works: TF-IDF = Term Frequency Ã— Inverse Document Frequency\n",
    "# - TF: How often a word appears in a document\n",
    "# - IDF: How rare a word is across all documents\n",
    "# - Rare words in specific documents get higher weights\n",
    "# Example: \"love\" in many reviews = lower weight, \"exceptional\" in few reviews = higher weight\n",
    "print(\"\\n=== TF-IDF VECTORIZATION ===\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,     # Same parameters as CountVectorizer for fair comparison\n",
    "    ngram_range=(1, 2),    # Include unigrams and bigrams\n",
    "    min_df=2,              # Ignore rare terms\n",
    "    max_df=0.8,            # Ignore too common terms\n",
    "    sublinear_tf=True      # Apply sublinear tf scaling (dampens effect of very high frequencies)\n",
    ")\n",
    "\n",
    "# Transform training and test data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF vectorizer vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"TF-IDF matrix shape - Train: {X_train_tfidf.shape}, Test: {X_test_tfidf.shape}\")\n",
    "\n",
    "# STEP 5: Feature Analysis\n",
    "# Show the vocabulary that was learned (most important words/phrases for analysis)\n",
    "print(\"\\n=== TOP FEATURES ===\")\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "print(\"Top 20 features by CountVectorizer:\")\n",
    "print(feature_names[:20])\n",
    "\n",
    "\"\"\"\n",
    "VECTORIZATION COMPARISON:\n",
    "- Count Vectorizer: Simple word frequency counting\n",
    "  * Pros: Simple, fast, good baseline\n",
    "  * Cons: Doesn't consider word importance across documents\n",
    "  \n",
    "- TF-IDF Vectorizer: Importance-weighted word frequency\n",
    "  * Pros: Considers word rarity, better for distinguishing documents\n",
    "  * Cons: Slightly more complex, can be sensitive to document collection\n",
    "\n",
    "NEXT STEPS:\n",
    "Both vectorized datasets (X_train_count, X_train_tfidf) will be used to train\n",
    "different machine learning models to compare which vectorization method works\n",
    "better for sentiment analysis on this specific dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b4bcf5",
   "metadata": {},
   "source": [
    "### 5.3 Traditional ML Model Training\n",
    "\n",
    "Training multiple traditional machine learning algorithms and comparing their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "077e1f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {np.str_('Negative'): np.float64(1.2184388856354054), np.str_('Neutral'): np.float64(4.062104871699517), np.str_('Positive'): np.float64(0.5524828344903456)}\n",
      "Before ROS: Counter({'Positive': 26361, 'Negative': 11953, 'Neutral': 5378})\n",
      "After ROS (Count): Counter({'Positive': 26361, 'Negative': 11953, 'Neutral': 11953})\n",
      "After ROS (TF-IDF): Counter({'Positive': 26361, 'Negative': 11953, 'Neutral': 11953})\n",
      "\n",
      "ðŸ”¢ TRAINING WITH COUNT VECTORIZATION (balanced train)\n",
      "\n",
      "--- Training Naive Bayes with Count ---\n",
      "Training Time: 0.06s  Acc: 0.5130  F1(w): 0.5299  F1(macro): 0.5310\n",
      "\n",
      "--- Training Logistic Regression with Count ---\n",
      "Training Time: 2.35s  Acc: 0.6955  F1(w): 0.7175  F1(macro): 0.6582\n",
      "\n",
      "--- Training SVM with Count ---\n",
      "Training Time: 12.84s  Acc: 0.7292  F1(w): 0.7427  F1(macro): 0.6716\n",
      "\n",
      "--- Training Random Forest with Count ---\n",
      "Training Time: 212.02s  Acc: 0.7985  F1(w): 0.7870  F1(macro): 0.7080\n",
      "\n",
      "--- Training XGBoost with Count ---\n",
      "Training Time: 4.01s  Acc: 0.5791  F1(w): 0.6071  F1(macro): 0.5845\n",
      "\n",
      "--- Training Gradient Boosting with Count ---\n",
      "Training Time: 58.71s  Acc: 0.7556  F1(w): 0.7477  F1(macro): 0.6514\n",
      "\n",
      "--- Training Extra Trees with Count ---\n",
      "Training Time: 1.04s  Acc: 0.3722  F1(w): 0.2803  F1(macro): 0.3673\n",
      "\n",
      "ðŸ“Š TRAINING WITH TF-IDF VECTORIZATION (balanced train)\n",
      "\n",
      "--- Training Naive Bayes with TF-IDF ---\n",
      "Training Time: 0.07s  Acc: 0.5923  F1(w): 0.6139  F1(macro): 0.5801\n",
      "\n",
      "--- Training Logistic Regression with TF-IDF ---\n",
      "Training Time: 1.24s  Acc: 0.7026  F1(w): 0.7248  F1(macro): 0.6674\n",
      "\n",
      "--- Training SVM with TF-IDF ---\n",
      "Training Time: 0.73s  Acc: 0.7398  F1(w): 0.7543  F1(macro): 0.6836\n",
      "\n",
      "--- Training Random Forest with TF-IDF ---\n",
      "Training Time: 231.66s  Acc: 0.8032  F1(w): 0.7931  F1(macro): 0.7157\n",
      "\n",
      "--- Training XGBoost with TF-IDF ---\n",
      "Training Time: 64.88s  Acc: 0.6166  F1(w): 0.6444  F1(macro): 0.6111\n",
      "\n",
      "--- Training Gradient Boosting with TF-IDF ---\n",
      "Training Time: 239.68s  Acc: 0.7624  F1(w): 0.7548  F1(macro): 0.6630\n",
      "\n",
      "--- Training Extra Trees with TF-IDF ---\n",
      "Training Time: 1.13s  Acc: 0.3689  F1(w): 0.2635  F1(macro): 0.3592\n",
      "\n",
      "âœ… All models trained successfully!\n",
      "Results stored in 'results' dictionary for further analysis.\n",
      "Balanced train class distribution (TF-IDF): Counter({'Positive': 26361, 'Negative': 11953, 'Neutral': 11953})\n",
      "Test class distribution: Counter({'Positive': 6590, 'Negative': 2988, 'Neutral': 1345})\n",
      "Top model by macro-F1: Random Forest\n",
      "Macro-F1: 0.7157\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TRADITIONAL ML MODELS TRAINING AND EVALUATION\n",
    "This cell trains multiple machine learning algorithms for sentiment classification, comparing their performance\n",
    "on the vectorized text data. We use both basic and advanced ensemble methods to find the best approach.\n",
    "ALGORITHM SELECTION RATIONALE:\n",
    "- Covers different ML paradigms: probabilistic, linear, kernel-based, and ensemble methods\n",
    "- Includes both traditional and modern high-performance algorithms\n",
    "- Allows comprehensive comparison to identify optimal approach for sentiment analysis\n",
    "\"\"\"\n",
    "\n",
    "# --- Imbalance handling: class weights and mild over-sampling ---\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "try:\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'imbalanced-learn'])\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Neutral class is often under-predicted, so we address this with class weights and mild over-sampling\n",
    "# Compute class weights from y_train\n",
    "class_names = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "cw = compute_class_weight(class_weight='balanced', classes=class_names, y=y_train.values)\n",
    "class_weight_dict = {cls: w for cls, w in zip(class_names, cw)}\n",
    "# Penalize missing Neutral predictions by increasing its weight\n",
    "class_weight_dict['Neutral'] *= 1.5\n",
    "print('Class weights:', class_weight_dict)\n",
    "\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial', solver='lbfgs', class_weight='balanced'),\n",
    "    'SVM': LinearSVC(random_state=42, max_iter=10000, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=400, class_weight='balanced_subsample', max_depth=None, min_samples_leaf=1, min_samples_split=2),\n",
    "    'XGBoost': XGBClassifier(random_state=42, n_estimators=200, max_depth=8, learning_rate=0.2, eval_metric='mlogloss', verbosity=0),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=6, learning_rate=0.1),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=42, n_estimators=100, max_depth=10, class_weight='balanced')\n",
    "}\n",
    "\n",
    "# Mild over-sampling: upsample Neutral to match Negative (keep Positive as is)\n",
    "counts = Counter(y_train)\n",
    "sampling_strategy = {\n",
    "    'Negative': counts['Negative'],\n",
    "    'Neutral': counts['Negative'],\n",
    "    'Positive': counts['Positive']\n",
    "}\n",
    "print('Before ROS:', counts)\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy=sampling_strategy)\n",
    "X_train_count_bal, y_train_count_bal = ros.fit_resample(X_train_count, y_train)\n",
    "X_train_tfidf_bal, y_train_tfidf_bal = ros.fit_resample(X_train_tfidf, y_train)\n",
    "print('After ROS (Count):', Counter(y_train_count_bal))\n",
    "print('After ROS (TF-IDF):', Counter(y_train_tfidf_bal))\n",
    "\n",
    "# Precompute sample_weight for XGBoost only (others use class_weight internally)\n",
    "xgb_sw_count = np.array([class_weight_dict[y] for y in y_train_count_bal])\n",
    "xgb_sw_tfidf = np.array([class_weight_dict[y] for y in y_train_tfidf_bal])\n",
    "\n",
    "results = {'Count': {}, 'TF-IDF': {}}\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name, vectorizer_name, sample_weight_train=None):\n",
    "    print(f\"\\n--- Training {model_name} with {vectorizer_name} ---\")\n",
    "    start_time = time.time()\n",
    "    if model_name == 'XGBoost' and sample_weight_train is not None:\n",
    "        # Encode y for XGB\n",
    "        le_local = LabelEncoder().fit(['Negative','Neutral','Positive'])\n",
    "        y_train_enc = le_local.transform(y_train)\n",
    "        model.fit(X_train, y_train_enc, sample_weight=sample_weight_train)\n",
    "        y_pred = le_local.inverse_transform(model.predict(X_test))\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    training_time = time.time() - start_time\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_w = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_w = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1_w = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    precision_per_class = precision_score(y_test, y_pred, average=None, labels=['Negative','Neutral','Positive'], zero_division=0)\n",
    "    recall_per_class = recall_score(y_test, y_pred, average=None, labels=['Negative','Neutral','Positive'], zero_division=0)\n",
    "    f1_per_class = f1_score(y_test, y_pred, average=None, labels=['Negative','Neutral','Positive'], zero_division=0)\n",
    "    results[vectorizer_name][model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision_w,\n",
    "        'recall': recall_w,\n",
    "        'f1': f1_w,\n",
    "        'f1_macro': f1_macro,\n",
    "        'training_time': training_time,\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    print(f\"Training Time: {training_time:.2f}s  Acc: {accuracy:.4f}  F1(w): {f1_w:.4f}  F1(macro): {f1_macro:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Train models with Count Vectorizer (balanced train)\n",
    "print(\"\\nðŸ”¢ TRAINING WITH COUNT VECTORIZATION (balanced train)\")\n",
    "trained_models_count = {}\n",
    "for model_name, model in models.items():\n",
    "    sw = xgb_sw_count if model_name == 'XGBoost' else None\n",
    "    trained_models_count[model_name] = evaluate_model(\n",
    "        model, X_train_count_bal, X_test_count, y_train_count_bal, y_test, model_name, 'Count', sample_weight_train=sw\n",
    ")\n",
    "\n",
    "# Train models with TF-IDF Vectorizer (balanced train)\n",
    "print(\"\\nðŸ“Š TRAINING WITH TF-IDF VECTORIZATION (balanced train)\")\n",
    "trained_models_tfidf = {}\n",
    "for model_name, model in models.items():\n",
    "    model_copy = model.__class__(**model.get_params())\n",
    "    sw = xgb_sw_tfidf if model_name == 'XGBoost' else None\n",
    "    trained_models_tfidf[model_name] = evaluate_model(\n",
    "        model_copy, X_train_tfidf_bal, X_test_tfidf, y_train_tfidf_bal, y_test, model_name, 'TF-IDF', sample_weight_train=sw\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… All models trained successfully!\")\n",
    "print(\"Results stored in 'results' dictionary for further analysis.\")\n",
    "\n",
    "# --- Sanity check: print class distributions and macro-F1 for top model ---\n",
    "print('Balanced train class distribution (TF-IDF):', Counter(y_train_tfidf_bal))\n",
    "print('Test class distribution:', Counter(y_test))\n",
    "# Find top model by macro-F1\n",
    "top_model = max(results['TF-IDF'], key=lambda m: results['TF-IDF'][m]['f1_macro'])\n",
    "print(f'Top model by macro-F1: {top_model}')\n",
    "print(f\"Macro-F1: {results['TF-IDF'][top_model]['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835bfce",
   "metadata": {},
   "source": [
    "\"\"\"## TF-IDF vs Count Vectorization: Understanding the Difference\n",
    "\n",
    "### What is TF-IDF?\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic that reflects how important a word is to a document within a collection of documents (corpus).\n",
    "\n",
    "### Mathematical Formula\n",
    "\n",
    "```\n",
    "TF-IDF(word, document) = TF(word, document) Ã— IDF(word, corpus)\n",
    "```\n",
    "\n",
    "**Where:**\n",
    "\n",
    "1. **TF (Term Frequency)** = (Number of times word appears in document) / (Total words in document)\n",
    "   - Measures how frequently a word appears in a specific document\n",
    "   - Higher TF = word appears more often in this document\n",
    "\n",
    "2. **IDF (Inverse Document Frequency)** = log(Total documents / Documents containing the word)\n",
    "   - Measures how rare or common a word is across all documents\n",
    "   - Higher IDF = word is rare across the corpus (more distinctive)\n",
    "   - Lower IDF = word is common across many documents (less distinctive)\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "Consider the word \"love\" in a customer review:\n",
    "- **Document**: \"I love this product, love the quality, amazing!\"\n",
    "- **TF**: \"love\" appears 2 times out of 8 words = 2/8 = 0.25\n",
    "- **IDF**: If \"love\" appears in 500 out of 1000 total reviews = log(1000/500) = 0.301\n",
    "- **TF-IDF**: 0.25 Ã— 0.301 = 0.075\n",
    "\n",
    "Compare with word \"exceptional\":\n",
    "- **TF**: \"exceptional\" appears 1 time out of 8 words = 1/8 = 0.125\n",
    "- **IDF**: If \"exceptional\" appears in only 10 out of 1000 reviews = log(1000/10) = 2.0\n",
    "- **TF-IDF**: 0.125 Ã— 2.0 = 0.25 *(Higher score despite lower frequency!)*\n",
    "\n",
    "### TF-IDF vs Count Vectorization\n",
    "\n",
    "| **Count Vectorizer** | **TF-IDF Vectorizer** |\n",
    "|---------------------|----------------------|\n",
    "| Simple word frequency counting | Importance-weighted word frequency |\n",
    "| Each word's value = how many times it appears | Considers both frequency AND rarity across documents |\n",
    "| Example: \"love\" appears 3 times â†’ value = 3 | Words common across all documents get lower weights |\n",
    "| Problem: Common words like \"the\", \"and\" get high scores but carry little meaning | Words unique to specific documents get higher weights |\n",
    "| | Better at identifying distinctive/meaningful words for classification |\n",
    "| | Automatically reduces impact of stop words without explicitly removing them |\n",
    "\n",
    "### Why Use TF-IDF for Sentiment Analysis?\n",
    "\n",
    "1. **Noise Reduction**: Automatically downweights common words that don't contribute to sentiment\n",
    "2. **Feature Importance**: Emphasizes words that are distinctive to specific sentiment categories\n",
    "3. **Better Classification**: Often leads to improved model performance for text classification tasks\n",
    "4. **Industry Standard**: Widely used baseline approach in NLP and information retrieval\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ccce1",
   "metadata": {},
   "source": [
    "### 5.4 Traditional ML Results Analysis\n",
    "\n",
    "Analyzing and visualizing the performance of traditional ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1d137ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRADITIONAL ML RESULTS SUMMARY ===\n",
      "Performance Comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Count</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.7590</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.5299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Count</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6955</td>\n",
       "      <td>0.7723</td>\n",
       "      <td>0.6955</td>\n",
       "      <td>0.7175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Count</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.7292</td>\n",
       "      <td>0.7665</td>\n",
       "      <td>0.7292</td>\n",
       "      <td>0.7427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Count</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>0.7949</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>0.7870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Count</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5791</td>\n",
       "      <td>0.7780</td>\n",
       "      <td>0.5791</td>\n",
       "      <td>0.6071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Count</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>0.7521</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>0.7477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Count</td>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.3722</td>\n",
       "      <td>0.7131</td>\n",
       "      <td>0.3722</td>\n",
       "      <td>0.2803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.5923</td>\n",
       "      <td>0.7294</td>\n",
       "      <td>0.5923</td>\n",
       "      <td>0.6139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.7026</td>\n",
       "      <td>0.7886</td>\n",
       "      <td>0.7026</td>\n",
       "      <td>0.7248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.7398</td>\n",
       "      <td>0.7810</td>\n",
       "      <td>0.7398</td>\n",
       "      <td>0.7543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.7931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6166</td>\n",
       "      <td>0.7811</td>\n",
       "      <td>0.6166</td>\n",
       "      <td>0.6444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.7624</td>\n",
       "      <td>0.7595</td>\n",
       "      <td>0.7624</td>\n",
       "      <td>0.7548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.3689</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>0.3689</td>\n",
       "      <td>0.2635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vectorizer                Model  Accuracy  Precision  Recall  F1-Score\n",
       "0       Count          Naive Bayes    0.5130     0.7590  0.5130    0.5299\n",
       "1       Count  Logistic Regression    0.6955     0.7723  0.6955    0.7175\n",
       "2       Count                  SVM    0.7292     0.7665  0.7292    0.7427\n",
       "3       Count        Random Forest    0.7985     0.7949  0.7985    0.7870\n",
       "4       Count              XGBoost    0.5791     0.7780  0.5791    0.6071\n",
       "5       Count    Gradient Boosting    0.7556     0.7521  0.7556    0.7477\n",
       "6       Count          Extra Trees    0.3722     0.7131  0.3722    0.2803\n",
       "7      TF-IDF          Naive Bayes    0.5923     0.7294  0.5923    0.6139\n",
       "8      TF-IDF  Logistic Regression    0.7026     0.7886  0.7026    0.7248\n",
       "9      TF-IDF                  SVM    0.7398     0.7810  0.7398    0.7543\n",
       "10     TF-IDF        Random Forest    0.8032     0.7980  0.8032    0.7931\n",
       "11     TF-IDF              XGBoost    0.6166     0.7811  0.6166    0.6444\n",
       "12     TF-IDF    Gradient Boosting    0.7624     0.7595  0.7624    0.7548\n",
       "13     TF-IDF          Extra Trees    0.3689     0.7053  0.3689    0.2635"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best performing model: Random Forest with TF-IDF vectorizer\n",
      "Best accuracy: 0.8032\n",
      "\n",
      "=== DETAILED RESULTS FOR BEST MODEL ===\n",
      "Model: Random Forest with TF-IDF vectorizer\n",
      "Overall Accuracy: 0.8032\n",
      "Overall Precision: 0.7980\n",
      "Overall Recall: 0.8032\n",
      "Overall F1-Score: 0.7931\n",
      "\n",
      "Per-class metrics:\n",
      "Negative:\n",
      "  Precision: 0.8377\n",
      "  Recall: 0.7600\n",
      "  F1-Score: 0.7970\n",
      "Neutral:\n",
      "  Precision: 0.6876\n",
      "  Recall: 0.3911\n",
      "  F1-Score: 0.4986\n",
      "Positive:\n",
      "  Precision: 0.8025\n",
      "  Recall: 0.9068\n",
      "  F1-Score: 0.8515\n",
      "\n",
      "=== CONFUSION MATRIX FOR BEST MODEL ===\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>2271</td>\n",
       "      <td>59</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>6</td>\n",
       "      <td>526</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>434</td>\n",
       "      <td>180</td>\n",
       "      <td>5976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Neutral  Positive\n",
       "Negative      2271       59       658\n",
       "Neutral          6      526       813\n",
       "Positive       434      180      5976"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Vectorizer=Count<br>Model=%{x}<br>Accuracy=%{y}<extra></extra>",
         "legendgroup": "Count",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Count",
         "offsetgroup": "Count",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Naive Bayes",
          "Logistic Regression",
          "SVM",
          "Random Forest",
          "XGBoost",
          "Gradient Boosting",
          "Extra Trees"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "fatBKt9q4D9OBtl8k0HmP0VW3VSRVec/NDVm5UyN6T9sIUrvWojiPz+24KSPLeg/QUVfXVDR1z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Vectorizer=TF-IDF<br>Model=%{x}<br>Accuracy=%{y}<extra></extra>",
         "legendgroup": "TF-IDF",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "TF-IDF",
         "offsetgroup": "TF-IDF",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Naive Bayes",
          "Logistic Regression",
          "SVM",
          "Random Forest",
          "XGBoost",
          "Gradient Boosting",
          "Extra Trees"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "aNFLF1r04j9M8NkHE3zmP0Oy3qaQrOc/Ms7mmIyz6T9i7M6JGLvjPz2XYTTPZeg/Q3NextCc1z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "legend": {
         "title": {
          "text": "Vectorizer"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Traditional ML Models Performance Comparison"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Model"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results analysis and comparison\n",
    "print(\"=== TRADITIONAL ML RESULTS SUMMARY ===\")\n",
    "\n",
    "# Create results comparison DataFrame\n",
    "comparison_data = []\n",
    "for vectorizer in ['Count', 'TF-IDF']:\n",
    "    for model_name in models.keys():\n",
    "        result = results[vectorizer][model_name]\n",
    "        comparison_data.append({\n",
    "            'Vectorizer': vectorizer,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'Precision': result['precision'],\n",
    "            'Recall': result['recall'],\n",
    "            'F1-Score': result['f1']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "display(results_df.round(4))\n",
    "\n",
    "# Find best performing model\n",
    "best_model = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "print(f\"\\nBest performing model: {best_model['Model']} with {best_model['Vectorizer']} vectorizer\")\n",
    "print(f\"Best accuracy: {best_model['Accuracy']:.4f}\")\n",
    "\n",
    "# Detailed results for best model\n",
    "best_vectorizer = best_model['Vectorizer']\n",
    "best_model_name = best_model['Model']\n",
    "best_result = results[best_vectorizer][best_model_name]\n",
    "\n",
    "print(f\"\\n=== DETAILED RESULTS FOR BEST MODEL ===\")\n",
    "print(f\"Model: {best_model_name} with {best_vectorizer} vectorizer\")\n",
    "print(f\"Overall Accuracy: {best_result['accuracy']:.4f}\")\n",
    "print(f\"Overall Precision: {best_result['precision']:.4f}\")\n",
    "print(f\"Overall Recall: {best_result['recall']:.4f}\")\n",
    "print(f\"Overall F1-Score: {best_result['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-class metrics:\")\n",
    "classes = ['Negative', 'Neutral', 'Positive']\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"{class_name}:\")\n",
    "    print(f\"  Precision: {best_result['precision_per_class'][i]:.4f}\")\n",
    "    print(f\"  Recall: {best_result['recall_per_class'][i]:.4f}\")\n",
    "    print(f\"  F1-Score: {best_result['f1_per_class'][i]:.4f}\")\n",
    "\n",
    "# Confusion Matrix for best model\n",
    "print(f\"\\n=== CONFUSION MATRIX FOR BEST MODEL ===\")\n",
    "best_y_pred = best_result['y_pred']\n",
    "cm = confusion_matrix(y_test, best_y_pred, labels=classes)\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "display(cm_df)\n",
    "\n",
    "# Visualize results\n",
    "try:\n",
    "    # Performance comparison plot\n",
    "    fig = px.bar(results_df, x='Model', y='Accuracy', color='Vectorizer',\n",
    "                 title='Traditional ML Models Performance Comparison',\n",
    "                 barmode='group')\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Plotly error: {e}\")\n",
    "    # Matplotlib fallback\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    models_list = results_df['Model'].unique()\n",
    "    x = np.arange(len(models_list))\n",
    "    width = 0.35\n",
    "    \n",
    "    count_accuracies = [results_df[(results_df['Model'] == model) & (results_df['Vectorizer'] == 'Count')]['Accuracy'].iloc[0] for model in models_list]\n",
    "    tfidf_accuracies = [results_df[(results_df['Model'] == model) & (results_df['Vectorizer'] == 'TF-IDF')]['Accuracy'].iloc[0] for model in models_list]\n",
    "    \n",
    "    plt.bar(x - width/2, count_accuracies, width, label='Count Vectorizer')\n",
    "    plt.bar(x + width/2, tfidf_accuracies, width, label='TF-IDF Vectorizer')\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Traditional ML Models Performance Comparison')\n",
    "    plt.xticks(x, models_list, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0c9e925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 27\u001b[0m\n\u001b[1;32m     13\u001b[0m xgb_param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]\n\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     19\u001b[0m xgb_grid \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     20\u001b[0m     XGBClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     21\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mxgb_param_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m  )\n\u001b[0;32m---> 27\u001b[0m \u001b[43mxgb_grid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf_bal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tfidf_bal_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_sw_tfidf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest XGBoost parameters:\u001b[39m\u001b[38;5;124m'\u001b[39m, xgb_grid\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest macro-F1:\u001b[39m\u001b[38;5;124m'\u001b[39m, xgb_grid\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/NLP Automated customers/project-nlp-automated-customer-reviews/.venv/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/NLP Automated customers/project-nlp-automated-customer-reviews/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1047\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/NLP Automated customers/project-nlp-automated-customer-reviews/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1605\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1605\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/NLP Automated customers/project-nlp-automated-customer-reviews/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:997\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    994\u001b[0m         )\n\u001b[1;32m    995\u001b[0m     )\n\u001b[0;32m--> 997\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1020\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/NLP Automated customers/project-nlp-automated-customer-reviews/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[1;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     75\u001b[0m     (\n\u001b[1;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/NLP Automated customers/project-nlp-automated-customer-reviews/.venv/lib/python3.10/site-packages/joblib/parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/NLP Automated customers/project-nlp-automated-customer-reviews/.venv/lib/python3.10/site-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/NLP Automated customers/project-nlp-automated-customer-reviews/.venv/lib/python3.10/site-packages/joblib/parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[1;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[0;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Grid Search for XGBoost hyperparameters ---\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    " \n",
    "le_local = LabelEncoder().fit(['Negative','Neutral','Positive'])\n",
    "\n",
    "\n",
    "# Reuse LabelEncoder from previous cells (assumed variable name: le_local)\n",
    "y_train_tfidf_bal_encoded = le_local.transform(y_train_tfidf_bal)\n",
    "y_test_encoded = le_local.transform(y_test)\n",
    " \n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.05, 0.1, 0.2]\n",
    "}\n",
    " \n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric='mlogloss', verbosity=0),\n",
    "    param_grid=xgb_param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    " )\n",
    " \n",
    "xgb_grid.fit(X_train_tfidf_bal, y_train_tfidf_bal_encoded, sample_weight=xgb_sw_tfidf)\n",
    " \n",
    "print('Best XGBoost parameters:', xgb_grid.best_params_)\n",
    "print('Best macro-F1:', xgb_grid.best_score_)\n",
    " \n",
    "# Predict and print confusion matrix for best XGBoost model\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "y_pred_xgb_encoded = best_xgb.predict(X_test_tfidf)\n",
    "y_pred_xgb = le_local.inverse_transform(y_pred_xgb_encoded)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "classes = ['Negative', 'Neutral', 'Positive']\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb, labels=classes)\n",
    "import pandas as pd\n",
    "cm_xgb_df = pd.DataFrame(cm_xgb, index=classes, columns=classes)\n",
    "print('Confusion Matrix for best XGBoost (TF-IDF):')\n",
    "display(cm_xgb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbffca78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RandomForest parameters: {'class_weight': 'balanced_subsample', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
      "Best macro-F1: 0.843941266537238\n",
      "Confusion Matrix for best RandomForest (TF-IDF):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>2271</td>\n",
       "      <td>59</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>6</td>\n",
       "      <td>526</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>434</td>\n",
       "      <td>180</td>\n",
       "      <td>5976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Neutral  Positive\n",
       "Negative      2271       59       658\n",
       "Neutral          6      526       813\n",
       "Positive       434      180      5976"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Grid Search for RandomForest hyperparameters (TF-IDF) ---\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    " \n",
    "rf_param_grid = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'max_depth': [None, 20, 40],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'class_weight': ['balanced_subsample']\n",
    "}\n",
    " \n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    " )\n",
    " \n",
    "rf_grid.fit(X_train_tfidf_bal, y_train_tfidf_bal)\n",
    " \n",
    "print('Best RandomForest parameters:', rf_grid.best_params_)\n",
    "print('Best macro-F1:', rf_grid.best_score_)\n",
    " \n",
    "# Predict and print confusion matrix for best RandomForest model\n",
    "best_rf = rf_grid.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test_tfidf)\n",
    "classes = ['Negative', 'Neutral', 'Positive']\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf, labels=classes)\n",
    "cm_rf_df = pd.DataFrame(cm_rf, index=classes, columns=classes)\n",
    "print('Confusion Matrix for best RandomForest (TF-IDF):')\n",
    "display(cm_rf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35fcf9",
   "metadata": {},
   "source": [
    "## STEP 6: Transformer Approach (HuggingFace)\n",
    "\n",
    "Implementing modern transformer-based models for sentiment classification using HuggingFace transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cac71",
   "metadata": {},
   "source": [
    "### 6.1 Pre-trained Model Selection and Baseline\n",
    "\n",
    "Testing pre-trained transformer models without fine-tuning to establish baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a2d7044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. DATA PREPROCESSING - HUGGINGFACE TRANSFORMERS ===\n",
      "ðŸŽ¯ TRANSFORMER MODELS FOR EVALUATION:\n",
      "   â€¢ BERT: bert-base-uncased\n",
      "   â€¢ RoBERTa: roberta-base\n",
      "   â€¢ DistilBERT: distilbert-base-uncased\n",
      "   â€¢ ELECTRA: google/electra-base-discriminator\n",
      "\n",
      "ðŸ“Š USING 3000 SAMPLES FOR TRANSFORMER PROCESSING\n",
      "   Train/Test Split: 80%/20%\n",
      "   Sentiment Distribution:\n",
      "sentiment\n",
      "Positive    1832\n",
      "Negative     789\n",
      "Neutral      379\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ”„ PREPROCESSING DATA FOR ALL MODELS...\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH BERT-BASE-UNCASED\n",
      "   Tokenizer: BertTokenizerFast\n",
      "   Vocabulary size: 30,522\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['i', 'bought', '2', 'of', 'these', ',', '1', 'for', 'each', 'of', 'my', '2', 'youngest', 'grand', '##au']...\n",
      "      Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [101, 1045, 4149, 1016, 1997, 2122, 1010, 1015, 2005, 2169, 1997, 2026, 1016, 6587, 2882, 4887, 13900, 2545, 1012, 2359]\n",
      "      Decoded back: [CLS] i bought 2 of these, 1 for each of my 2 youngest grandaughters. wanted\n",
      "âœ… BERT preprocessing completed\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH ROBERTA-BASE\n",
      "   Tokenizer: RobertaTokenizerFast\n",
      "   Vocabulary size: 50,265\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['I', 'Ä bought', 'Ä 2', 'Ä of', 'Ä these', ',', 'Ä 1', 'Ä for', 'Ä each', 'Ä of', 'Ä my', 'Ä 2', 'Ä youngest', 'Ä Grand', 'aughters']...\n",
      "      Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [0, 100, 2162, 132, 9, 209, 6, 112, 13, 349, 9, 127, 132, 8733, 2374, 28649, 4, 33149, 402, 14]\n",
      "      Decoded back: <s>I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that\n",
      "âœ… RoBERTa preprocessing completed\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH DISTILBERT-BASE-UNCASED\n",
      "   Tokenizer: DistilBertTokenizerFast\n",
      "   Vocabulary size: 30,522\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['i', 'bought', '2', 'of', 'these', ',', '1', 'for', 'each', 'of', 'my', '2', 'youngest', 'grand', '##au']...\n",
      "      Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [101, 1045, 4149, 1016, 1997, 2122, 1010, 1015, 2005, 2169, 1997, 2026, 1016, 6587, 2882, 4887, 13900, 2545, 1012, 2359]\n",
      "      Decoded back: [CLS] i bought 2 of these, 1 for each of my 2 youngest grandaughters. wanted\n",
      "âœ… DistilBERT preprocessing completed\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH GOOGLE/ELECTRA-BASE-DISCRIMINATOR\n",
      "   Tokenizer: ElectraTokenizerFast\n",
      "   Vocabulary size: 30,522\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['i', 'bought', '2', 'of', 'these', ',', '1', 'for', 'each', 'of', 'my', '2', 'youngest', 'grand', '##au']...\n",
      "      Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [101, 1045, 4149, 1016, 1997, 2122, 1010, 1015, 2005, 2169, 1997, 2026, 1016, 6587, 2882, 4887, 13900, 2545, 1012, 2359]\n",
      "      Decoded back: [CLS] i bought 2 of these, 1 for each of my 2 youngest grandaughters. wanted\n",
      "âœ… ELECTRA preprocessing completed\n",
      "\n",
      "âœ… DATA PREPROCESSING COMPLETED\n",
      "Successfully preprocessed data for 4 models\n",
      "Ready for model building and evaluation!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. DATA PREPROCESSING - HUGGINGFACE TRANSFORMERS\n",
    "\n",
    "This cell implements the complete HuggingFace transformer preprocessing pipeline as required:\n",
    "1.1 Data Cleaning and Tokenization - Using HuggingFace tokenizers\n",
    "1.2 Data Encoding - Converting text to numerical IDs\n",
    "\n",
    "WHAT ARE TRANSFORMER MODELS?\n",
    "Transformer models are a revolutionary deep learning architecture introduced in 2017 that use \n",
    "self-attention mechanisms to process sequential data like text. Unlike traditional ML approaches \n",
    "that work with hand-crafted features (like TF-IDF vectors), transformers learn complex patterns \n",
    "and contextual relationships directly from raw text.\n",
    "\n",
    "Attention is a mechanism that helps the model determine which parts of the input sequence are most relevant when processing a particular element.\n",
    "\n",
    "KEY TRANSFORMER CHARACTERISTICS:\n",
    "â€¢ Self-Attention: Can focus on different parts of the input text simultaneously\n",
    "â€¢ Contextual Understanding: Words get different representations based on surrounding context\n",
    "â€¢ Pre-training: Trained on massive text corpora to learn general language patterns\n",
    "â€¢ Transfer Learning: Can be fine-tuned for specific tasks like sentiment analysis\n",
    "â€¢ Bidirectional: Models like BERT read text in both directions for better context\n",
    "\n",
    "TRANSFORMER vs TRADITIONAL ML COMPARISON:\n",
    "Traditional ML (Previous Cells):     | Transformer Models (This Cell):\n",
    "â€¢ Manual feature engineering         | â€¢ Automatic feature learning\n",
    "â€¢ Fixed word representations         | â€¢ Dynamic contextual embeddings  \n",
    "â€¢ Bag-of-words assumptions          | â€¢ Sequential and positional awareness\n",
    "â€¢ Fast training/inference           | â€¢ Slower but more accurate\n",
    "â€¢ Interpretable features            | â€¢ Complex but powerful representations\n",
    "\n",
    "WHY THIS CELL COMES AFTER TRADITIONAL ML:\n",
    "1. PROGRESSIVE COMPLEXITY: We start with simpler, interpretable methods before advanced techniques\n",
    "2. BASELINE ESTABLISHMENT: Traditional ML provides performance benchmarks to beat\n",
    "3. COMPUTATIONAL EFFICIENCY: Traditional methods are faster, good for initial exploration\n",
    "4. EDUCATIONAL VALUE: Understanding both approaches shows evolution of NLP techniques\n",
    "5. PRACTICAL COMPARISON: Real projects need to evaluate speed vs accuracy trade-offs\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== 1. DATA PREPROCESSING - HUGGINGFACE TRANSFORMERS ===\")\n",
    "\n",
    "# Use the unified transformer_models from the top cell\n",
    "print(\"ðŸŽ¯ TRANSFORMER MODELS FOR EVALUATION:\")\n",
    "for name, model_id in transformer_models.items():\n",
    "    print(f\"   â€¢ {name}: {model_id}\")\n",
    "\n",
    "# Prepare sample data for transformer processing\n",
    "sample_size = min(3000, len(df_processed))  # Manageable size for transformers\n",
    "df_transformer_sample = df_processed.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nðŸ“Š USING {len(df_transformer_sample)} SAMPLES FOR TRANSFORMER PROCESSING\")\n",
    "print(f\"   Train/Test Split: 80%/20%\")\n",
    "print(f\"   Sentiment Distribution:\")\n",
    "print(df_transformer_sample['sentiment'].value_counts())\n",
    "\n",
    "# 1.1 DATA CLEANING AND TOKENIZATION using HuggingFace Transformers\n",
    "def huggingface_preprocessing(texts, labels, model_name, max_length=256):\n",
    "    \"\"\"\n",
    "    Complete HuggingFace preprocessing pipeline\n",
    "    \n",
    "    1.1 Data Cleaning and Tokenization:\n",
    "    - Clean text using HuggingFace tokenizer (handles special chars, punctuation)\n",
    "    - Apply model-specific tokenization (WordPiece, BPE, etc.)\n",
    "    - Add special tokens ([CLS], [SEP], [PAD])\n",
    "    \n",
    "    1.2 Data Encoding:\n",
    "    - Convert tokens to numerical IDs using tokenizer vocabulary\n",
    "    - Create attention masks for variable-length sequences\n",
    "    - Handle padding and truncation\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”§ PREPROCESSING WITH {model_name.upper()}\")\n",
    "    \n",
    "    # Load tokenizer for the specific model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"   Tokenizer: {tokenizer.__class__.__name__}\")\n",
    "    print(f\"   Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "    \n",
    "    # Convert sentiment labels to numerical format\n",
    "    label_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "    numerical_labels = [label_mapping[label] for label in labels]\n",
    "    \n",
    "    # Split data before tokenization\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "        texts, numerical_labels, test_size=0.2, random_state=42, stratify=numerical_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"   Train samples: {len(X_train_text)}\")\n",
    "    print(f\"   Test samples: {len(X_test_text)}\")\n",
    "    \n",
    "    # 1.1 TOKENIZATION: Convert text to tokens with cleaning\n",
    "    print(f\"   ðŸ”„ Tokenizing and cleaning data...\")\n",
    "    \n",
    "    # Show tokenization example BEFORE processing\n",
    "    sample_text = X_train_text[0][:100] + \"...\" if len(X_train_text[0]) > 100 else X_train_text[0]\n",
    "    sample_tokens = tokenizer.tokenize(sample_text)\n",
    "    \n",
    "    print(f\"\\n   ðŸ“ TOKENIZATION EXAMPLE:\")\n",
    "    print(f\"      Original text: {sample_text}\")\n",
    "    print(f\"      Tokens: {sample_tokens[:15]}...\")\n",
    "    print(f\"      Special tokens: {tokenizer.special_tokens_map}\")\n",
    "    \n",
    "    # 1.1 & 1.2 COMBINED: Tokenization + Encoding\n",
    "    train_encodings = tokenizer(\n",
    "        X_train_text,\n",
    "        truncation=True,          # Clean: truncate long sequences\n",
    "        padding=True,             # Clean: pad short sequences\n",
    "        max_length=max_length,    # Limit sequence length\n",
    "        return_tensors='pt',      # Return PyTorch tensors\n",
    "        return_attention_mask=True, # Create attention masks\n",
    "        add_special_tokens=True   # Add [CLS], [SEP] tokens\n",
    "    )\n",
    "    \n",
    "    test_encodings = tokenizer(\n",
    "        X_test_text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # 1.2 DATA ENCODING: Text â†’ Numerical IDs (completed by tokenizer)\n",
    "    print(f\"   âœ… Text cleaned and tokenized using HuggingFace tokenizer\")\n",
    "    print(f\"   âœ… Sequences encoded to numerical IDs from vocabulary\")\n",
    "    print(f\"   ðŸ“Š Input IDs shape: {train_encodings['input_ids'].shape}\")\n",
    "    print(f\"   ðŸ“Š Attention mask shape: {train_encodings['attention_mask'].shape}\")\n",
    "    \n",
    "    # Show encoding example\n",
    "    sample_ids = train_encodings['input_ids'][0][:20]\n",
    "    decoded_sample = tokenizer.decode(sample_ids, skip_special_tokens=False)\n",
    "    print(f\"      Encoded IDs: {sample_ids.tolist()}\")\n",
    "    print(f\"      Decoded back: {decoded_sample}\")\n",
    "    \n",
    "    return {\n",
    "        'tokenizer': tokenizer,\n",
    "        'train_encodings': train_encodings,\n",
    "        'test_encodings': test_encodings,\n",
    "        'y_train': torch.tensor(y_train),\n",
    "        'y_test': torch.tensor(y_test),\n",
    "        'X_train_text': X_train_text,\n",
    "        'X_test_text': X_test_text,\n",
    "        'label_mapping': label_mapping\n",
    "    }\n",
    "\n",
    "# Preprocess data for all transformer models\n",
    "transformer_data = {}\n",
    "texts = df_transformer_sample[text_column].astype(str).tolist()\n",
    "labels = df_transformer_sample['sentiment'].tolist()\n",
    "\n",
    "print(f\"\\nðŸ”„ PREPROCESSING DATA FOR ALL MODELS...\")\n",
    "\n",
    "for model_name, model_id in transformer_models.items():\n",
    "    try:\n",
    "        transformer_data[model_name] = huggingface_preprocessing(\n",
    "            texts, labels, model_id, max_length=256\n",
    "        )\n",
    "        print(f\"âœ… {model_name} preprocessing completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} preprocessing failed: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… DATA PREPROCESSING COMPLETED\")\n",
    "print(f\"Successfully preprocessed data for {len(transformer_data)} models\")\n",
    "print(f\"Ready for model building and evaluation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7caf7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using Apple Metal Performance Shaders (GPU)\n",
      "=== 2.1 MODEL SELECTION AND BASELINE PERFORMANCE ===\n",
      "ðŸŽ¯ MODEL SELECTION JUSTIFICATION:\n",
      "\n",
      "BERT (Bidirectional Encoder Representations from Transformers):\n",
      "âœ… Pioneering transformer architecture with bidirectional context\n",
      "âœ… Excellent baseline for most NLP tasks\n",
      "âœ… Multilingual variant handles diverse datasets\n",
      "âœ… Strong performance on sentiment classification\n",
      "âŒ Larger model size and slower inference\n",
      "âŒ Requires more computational resources\n",
      "\n",
      "RoBERTa (Robustly Optimized BERT Approach):\n",
      "âœ… Improved training methodology over BERT (no NSP, longer training)\n",
      "âœ… Better performance on downstream tasks\n",
      "âœ… Twitter variant optimized for social media text\n",
      "âœ… More robust to hyperparameters\n",
      "âŒ Requires significant computational resources\n",
      "âŒ Larger vocabulary than BERT\n",
      "\n",
      "DistilBERT (Distilled BERT):\n",
      "âœ… 60% smaller than BERT with 97% of performance\n",
      "âœ… 60% faster inference than BERT\n",
      "âœ… Good balance between speed and accuracy\n",
      "âœ… Easier deployment in production\n",
      "âŒ Slightly lower performance than full BERT\n",
      "âŒ May struggle with complex reasoning tasks\n",
      "\n",
      "ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately):\n",
      "âœ… More sample-efficient than BERT (learns from all tokens)\n",
      "âœ… Replaced token detection vs masked language modeling\n",
      "âœ… Better performance with same compute budget\n",
      "âœ… Discriminator-generator architecture innovation\n",
      "âŒ Newer architecture, less established\n",
      "âŒ No pre-trained sentiment models available\n",
      "\n",
      "\n",
      "ðŸ“Š BASELINE EVALUATION (Pre-trained models without fine-tuning)\n",
      "ðŸ” Evaluating BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing 500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7380\n",
      "      Precision: 0.7594\n",
      "      Recall: 0.7380\n",
      "      F1-Score: 0.7472\n",
      "ðŸ” Evaluating RoBERTa...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7380\n",
      "      Precision: 0.7594\n",
      "      Recall: 0.7380\n",
      "      F1-Score: 0.7472\n",
      "ðŸ” Evaluating RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing 500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7300\n",
      "      Precision: 0.7432\n",
      "      Recall: 0.7300\n",
      "      F1-Score: 0.7337\n",
      "ðŸ” Evaluating DistilBERT...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7300\n",
      "      Precision: 0.7432\n",
      "      Recall: 0.7300\n",
      "      F1-Score: 0.7337\n",
      "ðŸ” Evaluating DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing 500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7800\n",
      "      Precision: 0.7025\n",
      "      Recall: 0.7800\n",
      "      F1-Score: 0.7368\n",
      "ðŸ” Evaluating ELECTRA...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7800\n",
      "      Precision: 0.7025\n",
      "      Recall: 0.7800\n",
      "      F1-Score: 0.7368\n",
      "ðŸ” Evaluating ELECTRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing 500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.2720\n",
      "      Precision: 0.0740\n",
      "      Recall: 0.2720\n",
      "      F1-Score: 0.1163\n",
      "\n",
      "ðŸ“ˆ BASELINE RESULTS SUMMARY:\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.2720\n",
      "      Precision: 0.0740\n",
      "      Recall: 0.2720\n",
      "      F1-Score: 0.1163\n",
      "\n",
      "ðŸ“ˆ BASELINE RESULTS SUMMARY:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.7594</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.7472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.7432</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.7337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DistilBERT</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.7025</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.7368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ELECTRA</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.1163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Accuracy  Precision  Recall  F1-Score\n",
       "0        BERT     0.738     0.7594   0.738    0.7472\n",
       "1     RoBERTa     0.730     0.7432   0.730    0.7337\n",
       "2  DistilBERT     0.780     0.7025   0.780    0.7368\n",
       "3     ELECTRA     0.272     0.0740   0.272    0.1163"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† BEST BASELINE MODEL: DistilBERT\n",
      "   ðŸ“Š Baseline Accuracy: 0.7800\n",
      "   ðŸ“Š Baseline F1-Score: 0.7368\n",
      "   ðŸŽ¯ This is our benchmark to beat with fine-tuning!\n",
      "\n",
      "   ðŸ“‹ Per-class Performance:\n",
      "      Negative: P=0.674, R=0.897, F1=0.770\n",
      "      Neutral: P=0.000, R=0.000, F1=0.000\n",
      "      Positive: P=0.840, R=0.867, F1=0.854\n",
      "\n",
      "âœ… MODEL SELECTION BASELINE COMPLETED\n",
      "Successfully evaluated 4 baseline models\n",
      "Next step: Fine-tuning selected models for improved performance\n",
      "ðŸ§¹ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 MODEL SELECTION AND BASELINE PERFORMANCE\n",
    "\n",
    "This cell explores transformer-based models and evaluates their baseline performance without fine-tuning.\n",
    "We test multiple architectures to select the best pre-trained model for our sentiment analysis task.\n",
    "\"\"\"\n",
    "\n",
    "# Set device correctly for MacBook M4\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    device_id = 0\n",
    "    print(\"ðŸš€ Using Apple Metal Performance Shaders (GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_id = 0\n",
    "    print(\"ðŸš€ Using CUDA (GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_id = -1\n",
    "    print(\"ðŸ–¥ï¸ Using CPU\")\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"=== 2.1 MODEL SELECTION AND BASELINE PERFORMANCE ===\")\n",
    "\n",
    "# Pre-trained sentiment models for baseline testing\n",
    "baseline_sentiment_models = {\n",
    "    'BERT': 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "    'RoBERTa': 'cardiffnlp/twitter-roberta-base-sentiment-latest', \n",
    "    'DistilBERT': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    'ELECTRA': 'google/electra-small-discriminator'\n",
    "}\n",
    "\n",
    "print(\"ðŸŽ¯ MODEL SELECTION JUSTIFICATION:\")\n",
    "print(\"\"\"\n",
    "BERT (Bidirectional Encoder Representations from Transformers):\n",
    "âœ… Pioneering transformer architecture with bidirectional context\n",
    "âœ… Excellent baseline for most NLP tasks\n",
    "âœ… Multilingual variant handles diverse datasets\n",
    "âœ… Strong performance on sentiment classification\n",
    "âŒ Larger model size and slower inference\n",
    "âŒ Requires more computational resources\n",
    "\n",
    "RoBERTa (Robustly Optimized BERT Approach):\n",
    "âœ… Improved training methodology over BERT (no NSP, longer training)\n",
    "âœ… Better performance on downstream tasks\n",
    "âœ… Twitter variant optimized for social media text\n",
    "âœ… More robust to hyperparameters\n",
    "âŒ Requires significant computational resources\n",
    "âŒ Larger vocabulary than BERT\n",
    "\n",
    "DistilBERT (Distilled BERT):\n",
    "âœ… 60% smaller than BERT with 97% of performance\n",
    "âœ… 60% faster inference than BERT\n",
    "âœ… Good balance between speed and accuracy\n",
    "âœ… Easier deployment in production\n",
    "âŒ Slightly lower performance than full BERT\n",
    "âŒ May struggle with complex reasoning tasks\n",
    "\n",
    "ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately):\n",
    "âœ… More sample-efficient than BERT (learns from all tokens)\n",
    "âœ… Replaced token detection vs masked language modeling\n",
    "âœ… Better performance with same compute budget\n",
    "âœ… Discriminator-generator architecture innovation\n",
    "âŒ Newer architecture, less established\n",
    "âŒ No pre-trained sentiment models available\n",
    "\"\"\")\n",
    "\n",
    "def evaluate_baseline_model(model_name, model_id):\n",
    "    \"\"\"Evaluate a baseline pre-trained model\"\"\"\n",
    "    try:\n",
    "        print(f\"ðŸ” Evaluating {model_name}...\")\n",
    "        \n",
    "        # Create sentiment analysis pipeline\n",
    "        classifier = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=model_id, \n",
    "            tokenizer=model_id,\n",
    "            device=device_id,\n",
    "            return_all_scores=False,\n",
    "            truncation=True,\n",
    "            max_length=512,  # Fixed maximum length\n",
    "            padding=True     # Enable padding\n",
    "        )\n",
    "        \n",
    "        # Use smaller sample for baseline testing\n",
    "        # Check if variables exist, if not create fallback\n",
    "        try:\n",
    "            sample_texts = df_transformer_sample[text_column].astype(str).tolist()[:500]  # First 500 samples\n",
    "            sample_labels = df_transformer_sample['sentiment'].tolist()[:500]\n",
    "        except NameError:\n",
    "            # Fallback: use df if df_transformer_sample doesn't exist\n",
    "            try:\n",
    "                sample_texts = df[text_column].astype(str).tolist()[:500]\n",
    "                sample_labels = df['sentiment'].tolist()[:500]\n",
    "            except (NameError, KeyError):\n",
    "                print(f\"   âŒ Error: Required variables not found. Please ensure df_transformer_sample and text_column are defined.\")\n",
    "                return None\n",
    "        \n",
    "        \n",
    "        print(f\"   Processing {len(sample_texts)} samples...\")\n",
    "        \n",
    "        # Process one by one to avoid batch size issues\n",
    "        predictions = []\n",
    "        for i, text in enumerate(sample_texts):\n",
    "            try:\n",
    "                # Truncate very long texts manually\n",
    "                if len(text) > 1000:  # Truncate very long reviews\n",
    "                    text = text[:1000]\n",
    "                \n",
    "                pred = classifier(text)\n",
    "                predictions.append(pred[0] if isinstance(pred, list) else pred)\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"   Processed {i + 1}/{len(sample_texts)} samples...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   Warning: Sample {i+1} failed: {str(e)[:100]}...\")\n",
    "                # Add dummy prediction\n",
    "                predictions.append({'label': 'NEUTRAL', 'score': 0.5})\n",
    "        \n",
    "        \n",
    "        # Map predictions to our labels\n",
    "        predicted_labels = []\n",
    "        for pred in predictions:\n",
    "            label = str(pred['label']).upper()\n",
    "            if any(neg in label for neg in ['NEGATIVE', '1', '2', 'LABEL_0']):\n",
    "                predicted_labels.append('Negative')\n",
    "            elif any(neu in label for neu in ['NEUTRAL', '3', 'LABEL_1']):\n",
    "                predicted_labels.append('Neutral')\n",
    "            else:\n",
    "                predicted_labels.append('Positive')\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(sample_labels, predicted_labels)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            sample_labels, predicted_labels, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Per-class metrics\n",
    "        precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "            sample_labels, predicted_labels, average=None, \n",
    "            labels=['Negative', 'Neutral', 'Positive'], zero_division=0\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'precision_per_class': precision_per_class,\n",
    "            'recall_per_class': recall_per_class,\n",
    "            'f1_per_class': f1_per_class,\n",
    "            'predictions': predicted_labels,\n",
    "            'true_labels': sample_labels,\n",
    "            'model_type': 'baseline'\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… Baseline Results:\")\n",
    "        print(f\"      Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"      Precision: {precision:.4f}\")\n",
    "        print(f\"      Recall: {recall:.4f}\")\n",
    "        print(f\"      F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error evaluating {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluate baseline models\n",
    "print(f\"\\nðŸ“Š BASELINE EVALUATION (Pre-trained models without fine-tuning)\")\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for model_name, model_id in baseline_sentiment_models.items():\n",
    "    result = evaluate_baseline_model(model_name, model_id)  # âœ… Call the function\n",
    "    if result:\n",
    "        baseline_results[model_name] = result\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Skipping {model_name} due to evaluation error\")\n",
    "\n",
    "\n",
    "# Display baseline results summary\n",
    "if 'baseline_results' not in globals() or baseline_results is None:\n",
    "    baseline_results = {}\n",
    "if baseline_results:\n",
    "    print(f\"\\nðŸ“ˆ BASELINE RESULTS SUMMARY:\")\n",
    "    baseline_df = pd.DataFrame([\n",
    "        {\n",
    "            'Model': name,\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1-Score': results['f1']\n",
    "        }\n",
    "        for name, results in baseline_results.items()\n",
    "    ])\n",
    "    display(baseline_df.round(4))\n",
    "    if not baseline_df.empty:\n",
    "        best_baseline = baseline_df.loc[baseline_df['Accuracy'].idxmax()]\n",
    "        print(f\"\\nðŸ† BEST BASELINE MODEL: {best_baseline['Model']}\")\n",
    "        print(f\"   ðŸ“Š Baseline Accuracy: {best_baseline['Accuracy']:.4f}\")\n",
    "        print(f\"   ðŸ“Š Baseline F1-Score: {best_baseline['F1-Score']:.4f}\")\n",
    "        print(f\"   ðŸŽ¯ This is our benchmark to beat with fine-tuning!\")\n",
    "        best_results = baseline_results[best_baseline['Model']]\n",
    "        print(f\"\\n   ðŸ“‹ Per-class Performance:\")\n",
    "        classes = ['Negative', 'Neutral', 'Positive']\n",
    "        for i, class_name in enumerate(classes):\n",
    "            if i < len(best_results['precision_per_class']):\n",
    "                precision = best_results['precision_per_class'][i]\n",
    "                recall = best_results['recall_per_class'][i]\n",
    "                f1 = best_results['f1_per_class'][i]\n",
    "                print(f\"      {class_name}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
    "else:\n",
    "    print(\"No baseline results available.\")\n",
    "print(f\"\\nâœ… MODEL SELECTION BASELINE COMPLETED\")\n",
    "print(f\"Successfully evaluated {len(baseline_results)} baseline models\")\n",
    "print(f\"Next step: Fine-tuning selected models for improved performance\")\n",
    "\n",
    "# Clean up GPU memory\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    print(\"ðŸ§¹ GPU memory cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50f55a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE PERFORMANCE (WITHOUT FINE-TUNING) ===\n",
      "This is the performance using pre-trained models directly on our data:\n",
      "\n",
      "BERT (Pre-trained, no fine-tuning):\n",
      "   â€¢ Accuracy: 0.7380 (73.8%)\n",
      "   â€¢ F1-Score: 0.7472\n",
      "   â€¢ This is our baseline to compare against fine-tuned models\n",
      "\n",
      "RoBERTa (Pre-trained, no fine-tuning):\n",
      "   â€¢ Accuracy: 0.7300 (73.0%)\n",
      "   â€¢ F1-Score: 0.7337\n",
      "   â€¢ This is our baseline to compare against fine-tuned models\n",
      "\n",
      "DistilBERT (Pre-trained, no fine-tuning):\n",
      "   â€¢ Accuracy: 0.7800 (78.0%)\n",
      "   â€¢ F1-Score: 0.7368\n",
      "   â€¢ This is our baseline to compare against fine-tuned models\n",
      "\n",
      "ELECTRA (Pre-trained, no fine-tuning):\n",
      "   â€¢ Accuracy: 0.2720 (27.2%)\n",
      "   â€¢ F1-Score: 0.1163\n",
      "   â€¢ This is our baseline to compare against fine-tuned models\n"
     ]
    }
   ],
   "source": [
    "# Document baseline performance clearly\n",
    "print(\"=== BASELINE PERFORMANCE (WITHOUT FINE-TUNING) ===\")\n",
    "print(\"This is the performance using pre-trained models directly on our data:\")\n",
    "\n",
    "if 'baseline_results' not in globals() or baseline_results is None:\n",
    "    baseline_results = {}\n",
    "if baseline_results:\n",
    "    for model_name, results in baseline_results.items():\n",
    "        print(f\"\\n{model_name} (Pre-trained, no fine-tuning):\")\n",
    "        print(f\"   â€¢ Accuracy: {results['accuracy']:.4f} ({results['accuracy']:.1%})\")\n",
    "        print(f\"   â€¢ F1-Score: {results['f1']:.4f}\")\n",
    "        print(f\"   â€¢ This is our baseline to compare against fine-tuned models\")\n",
    "else:\n",
    "    print(\"No baseline results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7eebf60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3. MODEL EVALUATION ===\n",
      "=== 3.1 EVALUATION METRICS & 3.2 RESULTS ===\n",
      "ðŸ“Š COMPREHENSIVE TRANSFORMER EVALUATION RESULTS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Type</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT</td>\n",
       "      <td>Baseline (Pre-trained)</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.7594</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.7472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>Baseline (Pre-trained)</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.7432</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.7337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DistilBERT</td>\n",
       "      <td>Baseline (Pre-trained)</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.7025</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.7368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ELECTRA</td>\n",
       "      <td>Baseline (Pre-trained)</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.1163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model                    Type  Accuracy  Precision  Recall  F1-Score\n",
       "0        BERT  Baseline (Pre-trained)     0.738     0.7594   0.738    0.7472\n",
       "1     RoBERTa  Baseline (Pre-trained)     0.730     0.7432   0.730    0.7337\n",
       "2  DistilBERT  Baseline (Pre-trained)     0.780     0.7025   0.780    0.7368\n",
       "3     ELECTRA  Baseline (Pre-trained)     0.272     0.0740   0.272    0.1163"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† BEST PERFORMING TRANSFORMER MODEL:\n",
      "   ðŸ¥‡ Model: DistilBERT (Baseline (Pre-trained))\n",
      "   ðŸ“Š Accuracy: 0.7800 (78.0%)\n",
      "   ðŸ“Š F1-Score: 0.7368\n",
      "   ðŸ“Š Precision: 0.7025\n",
      "   ðŸ“Š Recall: 0.7800\n",
      "\n",
      "ðŸ“‹ DETAILED RESULTS - DISTILBERT (BASELINE (PRE-TRAINED)):\n",
      "   ðŸŽ¯ Model achieved an accuracy of 78.0% on the validation dataset\n",
      "\n",
      "   ðŸ“Š Per-class Performance:\n",
      "      â€¢ Class Negative:\n",
      "        - Precision: 67.4% (0.6740)\n",
      "        - Recall: 89.7% (0.8971)\n",
      "        - F1-score: 77.0% (0.7697)\n",
      "      â€¢ Class Neutral:\n",
      "        - Precision: 0.0% (0.0000)\n",
      "        - Recall: 0.0% (0.0000)\n",
      "        - F1-score: 0.0% (0.0000)\n",
      "      â€¢ Class Positive:\n",
      "        - Precision: 84.0% (0.8401)\n",
      "        - Recall: 86.7% (0.8673)\n",
      "        - F1-score: 85.4% (0.8535)\n",
      "\n",
      "ðŸ“Š CONFUSION MATRIX - DISTILBERT:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Label  Negative  Neutral  Positive\n",
       "True Label                                  \n",
       "Negative              122        0        14\n",
       "Neutral                18        0        37\n",
       "Positive               41        0       268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ DETAILED CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.90      0.77       136\n",
      "     Neutral       0.00      0.00      0.00        55\n",
      "    Positive       0.84      0.87      0.85       309\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.50      0.59      0.54       500\n",
      "weighted avg       0.70      0.78      0.74       500\n",
      "\n",
      "\n",
      "ðŸ“ˆ PERFORMANCE COMPARISON ANALYSIS:\n",
      "\n",
      "   ðŸ—ï¸ MODEL ARCHITECTURE COMPARISON:\n",
      "      BERT: Avg=0.7380, Max=0.7380\n",
      "      RoBERTa: Avg=0.7300, Max=0.7300\n",
      "      DistilBERT: Avg=0.7800, Max=0.7800\n",
      "      ELECTRA: Avg=0.2720, Max=0.2720\n",
      "\n",
      "ðŸ“Š CREATING COMPREHENSIVE VISUALIZATIONS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           "lightblue",
           "lightblue",
           "lightblue",
           "lightblue"
          ]
         },
         "name": "Accuracy",
         "type": "bar",
         "x": [
          "BERT (Baseline)",
          "RoBERTa (Baseline)",
          "DistilBERT (Baseline)",
          "ELECTRA (Baseline)"
         ],
         "xaxis": "x",
         "y": [
          0.738,
          0.73,
          0.78,
          0.272
         ],
         "yaxis": "y"
        },
        {
         "colorscale": [
          [
           0,
           "rgb(247,251,255)"
          ],
          [
           0.125,
           "rgb(222,235,247)"
          ],
          [
           0.25,
           "rgb(198,219,239)"
          ],
          [
           0.375,
           "rgb(158,202,225)"
          ],
          [
           0.5,
           "rgb(107,174,214)"
          ],
          [
           0.625,
           "rgb(66,146,198)"
          ],
          [
           0.75,
           "rgb(33,113,181)"
          ],
          [
           0.875,
           "rgb(8,81,156)"
          ],
          [
           1,
           "rgb(8,48,107)"
          ]
         ],
         "showscale": false,
         "text": {
          "bdata": "egAAAA4AEgAAACUAKQAAAAwB",
          "dtype": "i2",
          "shape": "3, 3"
         },
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          "Negative",
          "Neutral",
          "Positive"
         ],
         "xaxis": "x3",
         "y": [
          "Negative",
          "Neutral",
          "Positive"
         ],
         "yaxis": "y3",
         "z": {
          "bdata": "egAAAA4AEgAAACUAKQAAAAwB",
          "dtype": "i2",
          "shape": "3, 3"
         }
        },
        {
         "name": "Precision",
         "offsetgroup": "0",
         "opacity": 0.8,
         "type": "bar",
         "x": [
          "Negative",
          "Neutral",
          "Positive"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "MomD962R5T8AAAAAAAAAAKUNUaVO4uo/",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "name": "Recall",
         "offsetgroup": "1",
         "opacity": 0.8,
         "type": "bar",
         "x": [
          "Negative",
          "Neutral",
          "Positive"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "tbS0tLS07D8AAAAAAAAAAOkr/xwJwes/",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "name": "F1-Score",
         "offsetgroup": "2",
         "opacity": 0.8,
         "type": "bar",
         "x": [
          "Negative",
          "Neutral",
          "Positive"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "S19SooOh6D8AAAAAAAAAAIYGLOnlT+s/",
          "dtype": "f8"
         },
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Model Performance Comparison",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Baseline vs Fine-tuned",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Confusion Matrix (Best Model)",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Precision-Recall by Class",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 800,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "ðŸš€ Transformer Models Evaluation Dashboard<br>Best Model: DistilBERT (78.0% accuracy)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "tickangle": 45
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ],
         "tickangle": 45
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.375
         ]
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.375
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "3. MODEL EVALUATION\n",
    "\n",
    "3.1 Evaluation Metrics - Comprehensive performance evaluation\n",
    "3.2 Results - Detailed results presentation with confusion matrices\n",
    "\n",
    "This cell provides complete evaluation of both baseline and fine-tuned transformer models,\n",
    "comparing performance metrics and analyzing results across different sentiment classes.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== 3. MODEL EVALUATION ===\")\n",
    "print(\"=== 3.1 EVALUATION METRICS & 3.2 RESULTS ===\")\n",
    "\n",
    "# Combine all transformer results for comprehensive comparison\n",
    "all_transformer_results = []\n",
    "\n",
    "# Add baseline results (pre-trained models without fine-tuning)\n",
    "for model_name, results in baseline_results.items():\n",
    "    all_transformer_results.append({\n",
    "        'Model': f\"{model_name}\",\n",
    "        'Type': 'Baseline (Pre-trained)',\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1-Score': results['f1'],\n",
    "        'Details': results\n",
    "    })\n",
    "\"\"\"\n",
    "# Add fine-tuned results\n",
    "for model_name, results in fine_tuned_results.items():\n",
    "    all_transformer_results.append({\n",
    "        'Model': f\"{model_name}\",\n",
    "        'Type': 'Fine-tuned',\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1-Score': results['f1'],\n",
    "        'Details': results\n",
    "    })\"\"\"\n",
    "\n",
    "if all_transformer_results:\n",
    "    transformer_comparison_df = pd.DataFrame(all_transformer_results)\n",
    "    \n",
    "    print(\"ðŸ“Š COMPREHENSIVE TRANSFORMER EVALUATION RESULTS:\")\n",
    "    display_df = transformer_comparison_df.drop('Details', axis=1)  # Remove details for clean display\n",
    "    display(display_df.round(4))\n",
    "    \n",
    "    # Find best model overall\n",
    "    best_model_idx = transformer_comparison_df['Accuracy'].idxmax()\n",
    "    best_model = transformer_comparison_df.loc[best_model_idx]\n",
    "    \n",
    "    print(f\"\\nðŸ† BEST PERFORMING TRANSFORMER MODEL:\")\n",
    "    print(f\"   ðŸ¥‡ Model: {best_model['Model']} ({best_model['Type']})\")\n",
    "    print(f\"   ðŸ“Š Accuracy: {best_model['Accuracy']:.4f} ({best_model['Accuracy']:.1%})\")\n",
    "    print(f\"   ðŸ“Š F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "    print(f\"   ðŸ“Š Precision: {best_model['Precision']:.4f}\")\n",
    "    print(f\"   ðŸ“Š Recall: {best_model['Recall']:.4f}\")\n",
    "    \n",
    "    # Detailed evaluation for best model\n",
    "    best_results = best_model['Details']\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ DETAILED RESULTS - {best_model['Model'].upper()} ({best_model['Type'].upper()}):\")\n",
    "    print(f\"   ðŸŽ¯ Model achieved an accuracy of {best_results['accuracy']:.1%} on the validation dataset\")\n",
    "    \n",
    "    # Per-class performance\n",
    "    print(f\"\\n   ðŸ“Š Per-class Performance:\")\n",
    "    classes = ['Negative', 'Neutral', 'Positive']\n",
    "    \n",
    "    if len(best_results['precision_per_class']) >= 3:\n",
    "        for i, class_name in enumerate(classes):\n",
    "            precision = best_results['precision_per_class'][i]\n",
    "            recall = best_results['recall_per_class'][i] \n",
    "            f1 = best_results['f1_per_class'][i]\n",
    "            print(f\"      â€¢ Class {class_name}:\")\n",
    "            print(f\"        - Precision: {precision:.1%} ({precision:.4f})\")\n",
    "            print(f\"        - Recall: {recall:.1%} ({recall:.4f})\")\n",
    "            print(f\"        - F1-score: {f1:.1%} ({f1:.4f})\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    if 'confusion_matrix' in best_results:\n",
    "        cm = best_results['confusion_matrix']\n",
    "    else:\n",
    "        # Calculate confusion matrix if not stored\n",
    "        cm = confusion_matrix(\n",
    "            best_results['true_labels'], \n",
    "            best_results['predictions'], \n",
    "            labels=classes\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nðŸ“Š CONFUSION MATRIX - {best_model['Model'].upper()}:\")\n",
    "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    cm_df.index.name = 'True Label'\n",
    "    cm_df.columns.name = 'Predicted Label'\n",
    "    display(cm_df)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nðŸ“ˆ DETAILED CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(\n",
    "        best_results['true_labels'], \n",
    "        best_results['predictions'],\n",
    "        target_names=classes\n",
    "    ))\n",
    "    \n",
    "    # Training metrics for fine-tuned models\n",
    "    if best_model['Type'] == 'Fine-tuned':\n",
    "        print(f\"\\nðŸ‹ï¸ TRAINING METRICS:\")\n",
    "        print(f\"   Training Loss: {best_results['training_loss']:.4f}\")\n",
    "        print(f\"   Validation Loss: {best_results['eval_loss']:.4f}\")\n",
    "        \n",
    "        # Loss analysis\n",
    "        loss_ratio = best_results['eval_loss'] / best_results['training_loss']\n",
    "        if loss_ratio < 1.2:\n",
    "            print(f\"   ðŸ“Š Loss Ratio: {loss_ratio:.2f} (Good - No significant overfitting)\")\n",
    "        elif loss_ratio < 1.5:\n",
    "            print(f\"   ðŸ“Š Loss Ratio: {loss_ratio:.2f} (Acceptable - Slight overfitting)\")\n",
    "        else:\n",
    "            print(f\"   ðŸ“Š Loss Ratio: {loss_ratio:.2f} (Warning - Possible overfitting)\")\n",
    "\n",
    "# Performance comparison analysis\n",
    "print(f\"\\nðŸ“ˆ PERFORMANCE COMPARISON ANALYSIS:\")\n",
    "\"\"\"\n",
    "# Baseline vs Fine-tuned comparison\n",
    "baseline_models = set(baseline_results.keys())\n",
    "finetuned_models = set(fine_tuned_results.keys())\n",
    "common_models = baseline_models.intersection(finetuned_models)\n",
    "\n",
    "if common_models:\n",
    "    print(f\"\\n   ðŸ”„ FINE-TUNING IMPACT ANALYSIS:\")\n",
    "    for model_name in common_models:\n",
    "        baseline_acc = baseline_results[model_name]['accuracy']\n",
    "        finetuned_acc = fine_tuned_results[model_name]['accuracy']\n",
    "        improvement = finetuned_acc - baseline_acc\n",
    "        improvement_pct = (improvement / baseline_acc) * 100\n",
    "        \n",
    "    print(f\"      {model_name}:\")\n",
    "    print(f\"      â€¢ Baseline Accuracy: {baseline_acc:.4f}\")\n",
    "    print(f\"      â€¢ Fine-tuned Accuracy: {finetuned_acc:.4f}\")\n",
    "    print(f\"      â€¢ Improvement: {improvement:+.4f} ({improvement_pct:+.1f}%)\")\n",
    "        \n",
    "        if improvement > 0.05:\n",
    "            print(f\"      â€¢ ðŸš€ Significant improvement from fine-tuning!\")\n",
    "        elif improvement > 0.02:\n",
    "            print(f\"      â€¢ âœ… Moderate improvement from fine-tuning\")\n",
    "        elif improvement > 0:\n",
    "            print(f\"      â€¢ ðŸ“ˆ Small improvement from fine-tuning\")\n",
    "        else:\n",
    "            print(f\"      â€¢ âš ï¸ Fine-tuning did not improve performance\")\n",
    "\"\"\"\n",
    "# Model architecture comparison\n",
    "print(f\"\\n   ðŸ—ï¸ MODEL ARCHITECTURE COMPARISON:\")\n",
    "arch_comparison = {}\n",
    "for model_data in all_transformer_results:\n",
    "    model_base = model_data['Model'].split()[0]  # Get base model name\n",
    "    if model_base not in arch_comparison:\n",
    "        arch_comparison[model_base] = []\n",
    "    arch_comparison[model_base].append(model_data['Accuracy'])\n",
    "\n",
    "for arch, accuracies in arch_comparison.items():\n",
    "    avg_acc = np.mean(accuracies)\n",
    "    max_acc = np.max(accuracies)\n",
    "    print(f\"      {arch}: Avg={avg_acc:.4f}, Max={max_acc:.4f}\")\n",
    "\n",
    "# Comprehensive visualization\n",
    "print(f\"\\nðŸ“Š CREATING COMPREHENSIVE VISUALIZATIONS...\")\n",
    "\n",
    "try:\n",
    "    # Create comprehensive evaluation dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Model Performance Comparison',\n",
    "            'Baseline vs Fine-tuned', \n",
    "            'Confusion Matrix (Best Model)',\n",
    "            'Precision-Recall by Class'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "            [{\"type\": \"heatmap\"}, {\"type\": \"bar\"}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Model performance comparison\n",
    "    models = [f\"{row['Model']} ({row['Type'][:8]})\" for row in all_transformer_results]\n",
    "    accuracies = [row['Accuracy'] for row in all_transformer_results]\n",
    "    colors = ['lightblue' if 'Baseline' in row['Type'] else 'lightcoral' for row in all_transformer_results]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=models, y=accuracies, name='Accuracy', marker_color=colors),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Baseline vs Fine-tuned comparison\n",
    "    if baseline_results and fine_tuned_results:\n",
    "        comparison_data = []\n",
    "        comparison_labels = []\n",
    "        comparison_colors = []\n",
    "        \n",
    "        for model in baseline_results:\n",
    "            comparison_data.append(baseline_results[model]['accuracy'])\n",
    "            comparison_labels.append(f\"{model}\\nBaseline\")\n",
    "            comparison_colors.append('lightblue')\n",
    "            \n",
    "        for model in fine_tuned_results:\n",
    "            comparison_data.append(fine_tuned_results[model]['accuracy'])\n",
    "            comparison_labels.append(f\"{model}\\nFine-tuned\")\n",
    "            comparison_colors.append('lightcoral')\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=comparison_labels, y=comparison_data, name='Comparison', \n",
    "                  marker_color=comparison_colors, showlegend=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Confusion matrix heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=cm, x=classes, y=classes, colorscale='Blues', \n",
    "                  text=cm, texttemplate=\"%{text}\", showscale=False),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Precision-Recall by class for best model\n",
    "    if len(best_results['precision_per_class']) >= 3:\n",
    "        metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "        values = [\n",
    "            best_results['precision_per_class'],\n",
    "            best_results['recall_per_class'],\n",
    "            best_results['f1_per_class']\n",
    "        ]\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=classes, y=values[i], name=metric, \n",
    "                      offsetgroup=i, opacity=0.8),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800, \n",
    "        title_text=f\"ðŸš€ Transformer Models Evaluation Dashboard<br>Best Model: {best_model['Model']} ({best_model['Accuracy']:.1%} accuracy)\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "    fig.update_xaxes(tickangle=45, row=1, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "    \n",
    "    # Matplotlib fallback\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Subplot 1: Model comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    model_names = [f\"{row['Model']}\\n({row['Type'][:8]})\" for row in all_transformer_results]\n",
    "    accuracies = [row['Accuracy'] for row in all_transformer_results]\n",
    "    colors = ['lightblue' if 'Baseline' in row['Type'] else 'lightcoral' for row in all_transformer_results]\n",
    "    \n",
    "    plt.bar(range(len(model_names)), accuracies, color=colors)\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xticks(range(len(model_names)), model_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Subplot 2: Confusion matrix\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Confusion Matrix - {best_model[\"Model\"]}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Subplot 3: Per-class metrics\n",
    "    plt.subplot(2, 2, 3)\n",
    "    if len(best_results['precision_per_class']) >= 3:\n",
    "        x = np.arange(len(classes))\n",
    "        width = 0.25\n",
    "        \n",
    "        plt.bar(x - width, best_results['precision_per_class'], width, label='Precision', alpha=0.8)\n",
    "        plt.bar(x, best_results['recall_per_class'], width, label='Recall', alpha=0.8)\n",
    "        plt.bar(x + width, best_results['f1_per_class'], width, label='F1-Score', alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Per-class Performance Metrics')\n",
    "        plt.xticks(x, classes)\n",
    "        plt.legend()\n",
    "    \n",
    "    # Subplot 4: Accuracy distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(accuracies, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(best_model['Accuracy'], color='red', linestyle='--', \n",
    "                label=f'Best: {best_model[\"Accuracy\"]:.3f}')\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Accuracy Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c9877",
   "metadata": {},
   "source": [
    "## STEP 6: Transformer Approach (HuggingFace)\n",
    "\n",
    "Implementing modern transformer-based models for sentiment classification using HuggingFace transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fb3fd",
   "metadata": {},
   "source": [
    "### 6.1 Pre-trained Model Selection and Baseline\n",
    "\n",
    "Testing pre-trained transformer models without fine-tuning to establish baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96effe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. DATA PREPROCESSING - HUGGINGFACE TRANSFORMERS ===\n",
      "ðŸŽ¯ TRANSFORMER MODELS FOR EVALUATION:\n",
      "   â€¢ BERT: bert-base-uncased\n",
      "   â€¢ RoBERTa: roberta-base\n",
      "   â€¢ DistilBERT: distilbert-base-uncased\n",
      "   â€¢ ELECTRA: google/electra-base-discriminator\n",
      "\n",
      "ðŸ’¡ WHY ELECTRA WAS ADDED:\n",
      "   âœ… Efficient Pre-training: Uses replaced token detection instead of masked language modeling\n",
      "   âœ… Better Sample Efficiency: Learns from all input tokens, not just masked ones\n",
      "   âœ… Strong Performance: Often matches or exceeds BERT with less compute\n",
      "   âœ… Google Research: Advanced discriminator-generator architecture\n",
      "   âœ… Computational Efficiency: Faster training and inference than BERT\n",
      "\n",
      "ðŸ“Š USING 3000 SAMPLES FOR TRANSFORMER PROCESSING\n",
      "   Train/Test Split: 80%/20%\n",
      "   Sentiment Distribution:\n",
      "sentiment\n",
      "Positive    1832\n",
      "Negative     789\n",
      "Neutral      379\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“Š USING 3000 SAMPLES FOR TRANSFORMER PROCESSING\n",
      "   Train/Test Split: 80%/20%\n",
      "   Sentiment Distribution:\n",
      "sentiment\n",
      "Positive    1832\n",
      "Negative     789\n",
      "Neutral      379\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ”„ PREPROCESSING DATA FOR ALL MODELS...\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH BERT-BASE-UNCASED\n",
      "\n",
      "ðŸ”„ PREPROCESSING DATA FOR ALL MODELS...\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH BERT-BASE-UNCASED\n",
      "   Tokenizer: BertTokenizerFast\n",
      "   Vocabulary size: 30,522\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['i', 'bought', '2', 'of', 'these', ',', '1', 'for', 'each', 'of', 'my', '2', 'youngest', 'grand', '##au']...\n",
      "      Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "   Tokenizer: BertTokenizerFast\n",
      "   Vocabulary size: 30,522\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['i', 'bought', '2', 'of', 'these', ',', '1', 'for', 'each', 'of', 'my', '2', 'youngest', 'grand', '##au']...\n",
      "      Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [101, 1045, 4149, 1016, 1997, 2122, 1010, 1015, 2005, 2169, 1997, 2026, 1016, 6587, 2882, 4887, 13900, 2545, 1012, 2359]\n",
      "      Decoded back: [CLS] i bought 2 of these, 1 for each of my 2 youngest grandaughters. wanted\n",
      "âœ… BERT preprocessing completed\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH ROBERTA-BASE\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [101, 1045, 4149, 1016, 1997, 2122, 1010, 1015, 2005, 2169, 1997, 2026, 1016, 6587, 2882, 4887, 13900, 2545, 1012, 2359]\n",
      "      Decoded back: [CLS] i bought 2 of these, 1 for each of my 2 youngest grandaughters. wanted\n",
      "âœ… BERT preprocessing completed\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH ROBERTA-BASE\n",
      "   Tokenizer: RobertaTokenizerFast\n",
      "   Vocabulary size: 50,265\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['I', 'Ä bought', 'Ä 2', 'Ä of', 'Ä these', ',', 'Ä 1', 'Ä for', 'Ä each', 'Ä of', 'Ä my', 'Ä 2', 'Ä youngest', 'Ä Grand', 'aughters']...\n",
      "      Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "   Tokenizer: RobertaTokenizerFast\n",
      "   Vocabulary size: 50,265\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['I', 'Ä bought', 'Ä 2', 'Ä of', 'Ä these', ',', 'Ä 1', 'Ä for', 'Ä each', 'Ä of', 'Ä my', 'Ä 2', 'Ä youngest', 'Ä Grand', 'aughters']...\n",
      "      Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [0, 100, 2162, 132, 9, 209, 6, 112, 13, 349, 9, 127, 132, 8733, 2374, 28649, 4, 33149, 402, 14]\n",
      "      Decoded back: <s>I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that\n",
      "âœ… RoBERTa preprocessing completed\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH DISTILBERT-BASE-UNCASED\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [0, 100, 2162, 132, 9, 209, 6, 112, 13, 349, 9, 127, 132, 8733, 2374, 28649, 4, 33149, 402, 14]\n",
      "      Decoded back: <s>I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that\n",
      "âœ… RoBERTa preprocessing completed\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH DISTILBERT-BASE-UNCASED\n",
      "   Tokenizer: DistilBertTokenizerFast\n",
      "   Vocabulary size: 30,522\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['i', 'bought', '2', 'of', 'these', ',', '1', 'for', 'each', 'of', 'my', '2', 'youngest', 'grand', '##au']...\n",
      "      Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "   Tokenizer: DistilBertTokenizerFast\n",
      "   Vocabulary size: 30,522\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['i', 'bought', '2', 'of', 'these', ',', '1', 'for', 'each', 'of', 'my', '2', 'youngest', 'grand', '##au']...\n",
      "      Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [101, 1045, 4149, 1016, 1997, 2122, 1010, 1015, 2005, 2169, 1997, 2026, 1016, 6587, 2882, 4887, 13900, 2545, 1012, 2359]\n",
      "      Decoded back: [CLS] i bought 2 of these, 1 for each of my 2 youngest grandaughters. wanted\n",
      "âœ… DistilBERT preprocessing completed\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH GOOGLE/ELECTRA-BASE-DISCRIMINATOR\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [101, 1045, 4149, 1016, 1997, 2122, 1010, 1015, 2005, 2169, 1997, 2026, 1016, 6587, 2882, 4887, 13900, 2545, 1012, 2359]\n",
      "      Decoded back: [CLS] i bought 2 of these, 1 for each of my 2 youngest grandaughters. wanted\n",
      "âœ… DistilBERT preprocessing completed\n",
      "\n",
      "ðŸ”§ PREPROCESSING WITH GOOGLE/ELECTRA-BASE-DISCRIMINATOR\n",
      "   Tokenizer: ElectraTokenizerFast\n",
      "   Vocabulary size: 30,522\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['i', 'bought', '2', 'of', 'these', ',', '1', 'for', 'each', 'of', 'my', '2', 'youngest', 'grand', '##au']...\n",
      "      Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "   Tokenizer: ElectraTokenizerFast\n",
      "   Vocabulary size: 30,522\n",
      "   Train samples: 2400\n",
      "   Test samples: 600\n",
      "   ðŸ”„ Tokenizing and cleaning data...\n",
      "\n",
      "   ðŸ“ TOKENIZATION EXAMPLE:\n",
      "      Original text: I bought 2 of these, 1 for each of my 2 youngest Grandaughters. Wanted something that would be made ...\n",
      "      Tokens: ['i', 'bought', '2', 'of', 'these', ',', '1', 'for', 'each', 'of', 'my', '2', 'youngest', 'grand', '##au']...\n",
      "      Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [101, 1045, 4149, 1016, 1997, 2122, 1010, 1015, 2005, 2169, 1997, 2026, 1016, 6587, 2882, 4887, 13900, 2545, 1012, 2359]\n",
      "      Decoded back: [CLS] i bought 2 of these, 1 for each of my 2 youngest grandaughters. wanted\n",
      "âœ… ELECTRA preprocessing completed\n",
      "\n",
      "âœ… DATA PREPROCESSING COMPLETED\n",
      "Successfully preprocessed data for 4 models\n",
      "Ready for model building and evaluation!\n",
      "   âœ… Text cleaned and tokenized using HuggingFace tokenizer\n",
      "   âœ… Sequences encoded to numerical IDs from vocabulary\n",
      "   ðŸ“Š Input IDs shape: torch.Size([2400, 256])\n",
      "   ðŸ“Š Attention mask shape: torch.Size([2400, 256])\n",
      "      Encoded IDs: [101, 1045, 4149, 1016, 1997, 2122, 1010, 1015, 2005, 2169, 1997, 2026, 1016, 6587, 2882, 4887, 13900, 2545, 1012, 2359]\n",
      "      Decoded back: [CLS] i bought 2 of these, 1 for each of my 2 youngest grandaughters. wanted\n",
      "âœ… ELECTRA preprocessing completed\n",
      "\n",
      "âœ… DATA PREPROCESSING COMPLETED\n",
      "Successfully preprocessed data for 4 models\n",
      "Ready for model building and evaluation!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. DATA PREPROCESSING - HUGGINGFACE TRANSFORMERS\n",
    "\n",
    "This cell implements the complete HuggingFace transformer preprocessing pipeline as required:\n",
    "1.1 Data Cleaning and Tokenization - Using HuggingFace tokenizers\n",
    "1.2 Data Encoding - Converting text to numerical IDs\n",
    "\n",
    "WHAT ARE TRANSFORMER MODELS?\n",
    "Transformer models are a revolutionary deep learning architecture introduced in 2017 that use \n",
    "self-attention mechanisms to process sequential data like text. Unlike traditional ML approaches \n",
    "that work with hand-crafted features (like TF-IDF vectors), transformers learn complex patterns \n",
    "and contextual relationships directly from raw text.\n",
    "\n",
    "Attention is a mechanism that helps the model determine which parts of the input sequence are most relevant when processing a particular element.\n",
    "\n",
    "KEY TRANSFORMER CHARACTERISTICS:\n",
    "â€¢ Self-Attention: Can focus on different parts of the input text simultaneously\n",
    "â€¢ Contextual Understanding: Words get different representations based on surrounding context\n",
    "â€¢ Pre-training: Trained on massive text corpora to learn general language patterns\n",
    "â€¢ Transfer Learning: Can be fine-tuned for specific tasks like sentiment analysis\n",
    "â€¢ Bidirectional: Models like BERT read text in both directions for better context\n",
    "\n",
    "TRANSFORMER vs TRADITIONAL ML COMPARISON:\n",
    "Traditional ML (Previous Cells):     | Transformer Models (This Cell):\n",
    "â€¢ Manual feature engineering         | â€¢ Automatic feature learning\n",
    "â€¢ Fixed word representations         | â€¢ Dynamic contextual embeddings  \n",
    "â€¢ Bag-of-words assumptions          | â€¢ Sequential and positional awareness\n",
    "â€¢ Fast training/inference           | â€¢ Slower but more accurate\n",
    "â€¢ Interpretable features            | â€¢ Complex but powerful representations\n",
    "\n",
    "WHY THIS CELL COMES AFTER TRADITIONAL ML:\n",
    "1. PROGRESSIVE COMPLEXITY: We start with simpler, interpretable methods before advanced techniques\n",
    "2. BASELINE ESTABLISHMENT: Traditional ML provides performance benchmarks to beat\n",
    "3. COMPUTATIONAL EFFICIENCY: Traditional methods are faster, good for initial exploration\n",
    "4. EDUCATIONAL VALUE: Understanding both approaches shows evolution of NLP techniques\n",
    "5. PRACTICAL COMPARISON: Real projects need to evaluate speed vs accuracy trade-offs\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== 1. DATA PREPROCESSING - HUGGINGFACE TRANSFORMERS ===\")\n",
    "\n",
    "# 1.1 & 1.2 - Define transformer models for preprocessing and evaluation\n",
    "transformer_models = {\n",
    "    'BERT': 'bert-base-uncased',\n",
    "    'RoBERTa': 'roberta-base', \n",
    "    'DistilBERT': 'distilbert-base-uncased',\n",
    "    'ELECTRA': 'google/electra-base-discriminator'  # NEW: Adding ELECTRA model\n",
    "}\n",
    "\n",
    "print(\"ðŸŽ¯ TRANSFORMER MODELS FOR EVALUATION:\")\n",
    "for name, model_id in transformer_models.items():\n",
    "    print(f\"   â€¢ {name}: {model_id}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ WHY ELECTRA WAS ADDED:\")\n",
    "print(f\"   âœ… Efficient Pre-training: Uses replaced token detection instead of masked language modeling\")\n",
    "print(f\"   âœ… Better Sample Efficiency: Learns from all input tokens, not just masked ones\")\n",
    "print(f\"   âœ… Strong Performance: Often matches or exceeds BERT with less compute\")\n",
    "print(f\"   âœ… Google Research: Advanced discriminator-generator architecture\")\n",
    "print(f\"   âœ… Computational Efficiency: Faster training and inference than BERT\")\n",
    "\n",
    "# Prepare sample data for transformer processing\n",
    "sample_size = min(3000, len(df_processed))  # Manageable size for transformers\n",
    "df_transformer_sample = df_processed.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nðŸ“Š USING {len(df_transformer_sample)} SAMPLES FOR TRANSFORMER PROCESSING\")\n",
    "print(f\"   Train/Test Split: 80%/20%\")\n",
    "print(f\"   Sentiment Distribution:\")\n",
    "print(df_transformer_sample['sentiment'].value_counts())\n",
    "\n",
    "# 1.1 DATA CLEANING AND TOKENIZATION using HuggingFace Transformers\n",
    "def huggingface_preprocessing(texts, labels, model_name, max_length=256):\n",
    "    \"\"\"\n",
    "    Complete HuggingFace preprocessing pipeline\n",
    "    \n",
    "    1.1 Data Cleaning and Tokenization:\n",
    "    - Clean text using HuggingFace tokenizer (handles special chars, punctuation)\n",
    "    - Apply model-specific tokenization (WordPiece, BPE, etc.)\n",
    "    - Add special tokens ([CLS], [SEP], [PAD])\n",
    "    \n",
    "    1.2 Data Encoding:\n",
    "    - Convert tokens to numerical IDs using tokenizer vocabulary\n",
    "    - Create attention masks for variable-length sequences\n",
    "    - Handle padding and truncation\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”§ PREPROCESSING WITH {model_name.upper()}\")\n",
    "    \n",
    "    # Load tokenizer for the specific model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"   Tokenizer: {tokenizer.__class__.__name__}\")\n",
    "    print(f\"   Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "    \n",
    "    # Convert sentiment labels to numerical format\n",
    "    label_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "    numerical_labels = [label_mapping[label] for label in labels]\n",
    "    \n",
    "    # Split data before tokenization\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "        texts, numerical_labels, test_size=0.2, random_state=42, stratify=numerical_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"   Train samples: {len(X_train_text)}\")\n",
    "    print(f\"   Test samples: {len(X_test_text)}\")\n",
    "    \n",
    "    # 1.1 TOKENIZATION: Convert text to tokens with cleaning\n",
    "    print(f\"   ðŸ”„ Tokenizing and cleaning data...\")\n",
    "    \n",
    "    # Show tokenization example BEFORE processing\n",
    "    sample_text = X_train_text[0][:100] + \"...\" if len(X_train_text[0]) > 100 else X_train_text[0]\n",
    "    sample_tokens = tokenizer.tokenize(sample_text)\n",
    "    \n",
    "    print(f\"\\n   ðŸ“ TOKENIZATION EXAMPLE:\")\n",
    "    print(f\"      Original text: {sample_text}\")\n",
    "    print(f\"      Tokens: {sample_tokens[:15]}...\")\n",
    "    print(f\"      Special tokens: {tokenizer.special_tokens_map}\")\n",
    "    \n",
    "    # 1.1 & 1.2 COMBINED: Tokenization + Encoding\n",
    "    train_encodings = tokenizer(\n",
    "        X_train_text,\n",
    "        truncation=True,          # Clean: truncate long sequences\n",
    "        padding=True,             # Clean: pad short sequences\n",
    "        max_length=max_length,    # Limit sequence length\n",
    "        return_tensors='pt',      # Return PyTorch tensors\n",
    "        return_attention_mask=True, # Create attention masks\n",
    "        add_special_tokens=True   # Add [CLS], [SEP] tokens\n",
    "    )\n",
    "    \n",
    "    test_encodings = tokenizer(\n",
    "        X_test_text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # 1.2 DATA ENCODING: Text â†’ Numerical IDs (completed by tokenizer)\n",
    "    print(f\"   âœ… Text cleaned and tokenized using HuggingFace tokenizer\")\n",
    "    print(f\"   âœ… Sequences encoded to numerical IDs from vocabulary\")\n",
    "    print(f\"   ðŸ“Š Input IDs shape: {train_encodings['input_ids'].shape}\")\n",
    "    print(f\"   ðŸ“Š Attention mask shape: {train_encodings['attention_mask'].shape}\")\n",
    "    \n",
    "    # Show encoding example\n",
    "    sample_ids = train_encodings['input_ids'][0][:20]\n",
    "    decoded_sample = tokenizer.decode(sample_ids, skip_special_tokens=False)\n",
    "    print(f\"      Encoded IDs: {sample_ids.tolist()}\")\n",
    "    print(f\"      Decoded back: {decoded_sample}\")\n",
    "    \n",
    "    return {\n",
    "        'tokenizer': tokenizer,\n",
    "        'train_encodings': train_encodings,\n",
    "        'test_encodings': test_encodings,\n",
    "        'y_train': torch.tensor(y_train),\n",
    "        'y_test': torch.tensor(y_test),\n",
    "        'X_train_text': X_train_text,\n",
    "        'X_test_text': X_test_text,\n",
    "        'label_mapping': label_mapping\n",
    "    }\n",
    "\n",
    "# Preprocess data for all transformer models\n",
    "transformer_data = {}\n",
    "texts = df_transformer_sample[text_column].astype(str).tolist()\n",
    "labels = df_transformer_sample['sentiment'].tolist()\n",
    "\n",
    "print(f\"\\nðŸ”„ PREPROCESSING DATA FOR ALL MODELS...\")\n",
    "\n",
    "for model_name, model_id in transformer_models.items():\n",
    "    try:\n",
    "        transformer_data[model_name] = huggingface_preprocessing(\n",
    "            texts, labels, model_id, max_length=256\n",
    "        )\n",
    "        print(f\"âœ… {model_name} preprocessing completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} preprocessing failed: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… DATA PREPROCESSING COMPLETED\")\n",
    "print(f\"Successfully preprocessed data for {len(transformer_data)} models\")\n",
    "print(f\"Ready for model building and evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db1f592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using Apple Metal Performance Shaders (GPU)\n",
      "=== 2.1 MODEL SELECTION AND BASELINE PERFORMANCE ===\n",
      "ðŸŽ¯ MODEL SELECTION JUSTIFICATION:\n",
      "\n",
      "BERT (Bidirectional Encoder Representations from Transformers):\n",
      "âœ… Pioneering transformer architecture with bidirectional context\n",
      "âœ… Excellent baseline for most NLP tasks\n",
      "âœ… Multilingual variant handles diverse datasets\n",
      "âœ… Strong performance on sentiment classification\n",
      "âŒ Larger model size and slower inference\n",
      "âŒ Requires more computational resources\n",
      "\n",
      "RoBERTa (Robustly Optimized BERT Approach):\n",
      "âœ… Improved training methodology over BERT (no NSP, longer training)\n",
      "âœ… Better performance on downstream tasks\n",
      "âœ… Twitter variant optimized for social media text\n",
      "âœ… More robust to hyperparameters\n",
      "âŒ Requires significant computational resources\n",
      "âŒ Larger vocabulary than BERT\n",
      "\n",
      "DistilBERT (Distilled BERT):\n",
      "âœ… 60% smaller than BERT with 97% of performance\n",
      "âœ… 60% faster inference than BERT\n",
      "âœ… Good balance between speed and accuracy\n",
      "âœ… Easier deployment in production\n",
      "âŒ Slightly lower performance than full BERT\n",
      "âŒ May struggle with complex reasoning tasks\n",
      "\n",
      "ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately):\n",
      "âœ… More sample-efficient than BERT (learns from all tokens)\n",
      "âœ… Replaced token detection vs masked language modeling\n",
      "âœ… Better performance with same compute budget\n",
      "âœ… Discriminator-generator architecture innovation\n",
      "âŒ Newer architecture, less established\n",
      "âŒ No pre-trained sentiment models available\n",
      "\n",
      "\n",
      "ðŸ“Š BASELINE EVALUATION (Pre-trained models without fine-tuning)\n",
      "ðŸ” Evaluating BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing 500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7380\n",
      "      Precision: 0.7594\n",
      "      Recall: 0.7380\n",
      "      F1-Score: 0.7472\n",
      "ðŸ” Evaluating RoBERTa...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7380\n",
      "      Precision: 0.7594\n",
      "      Recall: 0.7380\n",
      "      F1-Score: 0.7472\n",
      "ðŸ” Evaluating RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing 500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7300\n",
      "      Precision: 0.7432\n",
      "      Recall: 0.7300\n",
      "      F1-Score: 0.7337\n",
      "ðŸ” Evaluating DistilBERT...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7300\n",
      "      Precision: 0.7432\n",
      "      Recall: 0.7300\n",
      "      F1-Score: 0.7337\n",
      "ðŸ” Evaluating DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing 500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 50/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 100/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 150/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 200/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 250/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 300/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 350/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 400/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 450/500 samples...\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7800\n",
      "      Precision: 0.7025\n",
      "      Recall: 0.7800\n",
      "      F1-Score: 0.7368\n",
      "\n",
      "ðŸ“ˆ BASELINE RESULTS SUMMARY:\n",
      "   Processed 500/500 samples...\n",
      "   âœ… Baseline Results:\n",
      "      Accuracy: 0.7800\n",
      "      Precision: 0.7025\n",
      "      Recall: 0.7800\n",
      "      F1-Score: 0.7368\n",
      "\n",
      "ðŸ“ˆ BASELINE RESULTS SUMMARY:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.7594</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.7472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.7432</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.7337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DistilBERT</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.7025</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.7368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Accuracy  Precision  Recall  F1-Score\n",
       "0        BERT     0.738     0.7594   0.738    0.7472\n",
       "1     RoBERTa     0.730     0.7432   0.730    0.7337\n",
       "2  DistilBERT     0.780     0.7025   0.780    0.7368"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† BEST BASELINE MODEL: DistilBERT\n",
      "   ðŸ“Š Baseline Accuracy: 0.7800\n",
      "   ðŸ“Š Baseline F1-Score: 0.7368\n",
      "   ðŸŽ¯ This is our benchmark to beat with fine-tuning!\n",
      "\n",
      "   ðŸ“‹ Per-class Performance:\n",
      "      Negative: P=0.674, R=0.897, F1=0.770\n",
      "      Neutral: P=0.000, R=0.000, F1=0.000\n",
      "      Positive: P=0.840, R=0.867, F1=0.854\n",
      "\n",
      "âœ… MODEL SELECTION BASELINE COMPLETED\n",
      "Successfully evaluated 3 baseline models\n",
      "Next step: Fine-tuning selected models for improved performance\n",
      "ðŸ§¹ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 MODEL SELECTION AND BASELINE PERFORMANCE\n",
    "\n",
    "This cell explores transformer-based models and evaluates their baseline performance without fine-tuning.\n",
    "We test multiple architectures to select the best pre-trained model for our sentiment analysis task.\n",
    "\"\"\"\n",
    "\n",
    "# Set device correctly for MacBook M4\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    device_id = 0\n",
    "    print(\"ðŸš€ Using Apple Metal Performance Shaders (GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_id = 0\n",
    "    print(\"ðŸš€ Using CUDA (GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_id = -1\n",
    "    print(\"ðŸ–¥ï¸ Using CPU\")\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"=== 2.1 MODEL SELECTION AND BASELINE PERFORMANCE ===\")\n",
    "\n",
    "# Pre-trained sentiment models for baseline testing\n",
    "baseline_sentiment_models = {\n",
    "    'BERT': 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "    'RoBERTa': 'cardiffnlp/twitter-roberta-base-sentiment-latest', \n",
    "    'DistilBERT': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    # ELECTRA doesn't have a pre-trained sentiment variant, so we'll evaluate it after fine-tuning\n",
    "}\n",
    "\n",
    "print(\"ðŸŽ¯ MODEL SELECTION JUSTIFICATION:\")\n",
    "print(\"\"\"\n",
    "BERT (Bidirectional Encoder Representations from Transformers):\n",
    "âœ… Pioneering transformer architecture with bidirectional context\n",
    "âœ… Excellent baseline for most NLP tasks\n",
    "âœ… Multilingual variant handles diverse datasets\n",
    "âœ… Strong performance on sentiment classification\n",
    "âŒ Larger model size and slower inference\n",
    "âŒ Requires more computational resources\n",
    "\n",
    "RoBERTa (Robustly Optimized BERT Approach):\n",
    "âœ… Improved training methodology over BERT (no NSP, longer training)\n",
    "âœ… Better performance on downstream tasks\n",
    "âœ… Twitter variant optimized for social media text\n",
    "âœ… More robust to hyperparameters\n",
    "âŒ Requires significant computational resources\n",
    "âŒ Larger vocabulary than BERT\n",
    "\n",
    "DistilBERT (Distilled BERT):\n",
    "âœ… 60% smaller than BERT with 97% of performance\n",
    "âœ… 60% faster inference than BERT\n",
    "âœ… Good balance between speed and accuracy\n",
    "âœ… Easier deployment in production\n",
    "âŒ Slightly lower performance than full BERT\n",
    "âŒ May struggle with complex reasoning tasks\n",
    "\n",
    "ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately):\n",
    "âœ… More sample-efficient than BERT (learns from all tokens)\n",
    "âœ… Replaced token detection vs masked language modeling\n",
    "âœ… Better performance with same compute budget\n",
    "âœ… Discriminator-generator architecture innovation\n",
    "âŒ Newer architecture, less established\n",
    "âŒ No pre-trained sentiment models available\n",
    "\"\"\")\n",
    "\n",
    "def evaluate_baseline_model(model_name, model_id):\n",
    "    \"\"\"Evaluate a baseline pre-trained model\"\"\"\n",
    "    try:\n",
    "        print(f\"ðŸ” Evaluating {model_name}...\")\n",
    "        \n",
    "        # Create sentiment analysis pipeline\n",
    "        classifier = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=model_id, \n",
    "            tokenizer=model_id,\n",
    "            device=device_id,\n",
    "            return_all_scores=False,\n",
    "            truncation=True,\n",
    "            max_length=512,  # Fixed maximum length\n",
    "            padding=True     # Enable padding\n",
    "        )\n",
    "        \n",
    "        # Use smaller sample for baseline testing\n",
    "        # Check if variables exist, if not create fallback\n",
    "        try:\n",
    "            sample_texts = df_transformer_sample[text_column].astype(str).tolist()[:500]  # First 500 samples\n",
    "            sample_labels = df_transformer_sample['sentiment'].tolist()[:500]\n",
    "        except NameError:\n",
    "            # Fallback: use df if df_transformer_sample doesn't exist\n",
    "            try:\n",
    "                sample_texts = df[text_column].astype(str).tolist()[:500]\n",
    "                sample_labels = df['sentiment'].tolist()[:500]\n",
    "            except (NameError, KeyError):\n",
    "                print(f\"   âŒ Error: Required variables not found. Please ensure df_transformer_sample and text_column are defined.\")\n",
    "                return None\n",
    "        \n",
    "        \n",
    "        print(f\"   Processing {len(sample_texts)} samples...\")\n",
    "        \n",
    "        # Process one by one to avoid batch size issues\n",
    "        predictions = []\n",
    "        for i, text in enumerate(sample_texts):\n",
    "            try:\n",
    "                # Truncate very long texts manually\n",
    "                if len(text) > 1000:  # Truncate very long reviews\n",
    "                    text = text[:1000]\n",
    "                \n",
    "                pred = classifier(text)\n",
    "                predictions.append(pred[0] if isinstance(pred, list) else pred)\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"   Processed {i + 1}/{len(sample_texts)} samples...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   Warning: Sample {i+1} failed: {str(e)[:100]}...\")\n",
    "                # Add dummy prediction\n",
    "                predictions.append({'label': 'NEUTRAL', 'score': 0.5})\n",
    "        \n",
    "        \n",
    "        # Map predictions to our labels\n",
    "        predicted_labels = []\n",
    "        for pred in predictions:\n",
    "            label = str(pred['label']).upper()\n",
    "            if any(neg in label for neg in ['NEGATIVE', '1', '2', 'LABEL_0']):\n",
    "                predicted_labels.append('Negative')\n",
    "            elif any(neu in label for neu in ['NEUTRAL', '3', 'LABEL_1']):\n",
    "                predicted_labels.append('Neutral')\n",
    "            else:\n",
    "                predicted_labels.append('Positive')\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(sample_labels, predicted_labels)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            sample_labels, predicted_labels, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Per-class metrics\n",
    "        precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "            sample_labels, predicted_labels, average=None, \n",
    "            labels=['Negative', 'Neutral', 'Positive'], zero_division=0\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'precision_per_class': precision_per_class,\n",
    "            'recall_per_class': recall_per_class,\n",
    "            'f1_per_class': f1_per_class,\n",
    "            'predictions': predicted_labels,\n",
    "            'true_labels': sample_labels,\n",
    "            'model_type': 'baseline'\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… Baseline Results:\")\n",
    "        print(f\"      Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"      Precision: {precision:.4f}\")\n",
    "        print(f\"      Recall: {recall:.4f}\")\n",
    "        print(f\"      F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error evaluating {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluate baseline models\n",
    "print(f\"\\nðŸ“Š BASELINE EVALUATION (Pre-trained models without fine-tuning)\")\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for model_name, model_id in baseline_sentiment_models.items():\n",
    "    result = evaluate_baseline_model(model_name, model_id)  # âœ… Call the function\n",
    "    if result:\n",
    "        baseline_results[model_name] = result\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Skipping {model_name} due to evaluation error\")\n",
    "\n",
    "# Display baseline results summary\n",
    "if baseline_results:\n",
    "    print(f\"\\nðŸ“ˆ BASELINE RESULTS SUMMARY:\")\n",
    "    baseline_df = pd.DataFrame([\n",
    "        {\n",
    "            'Model': name,\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1-Score': results['f1']\n",
    "        }\n",
    "        for name, results in baseline_results.items()\n",
    "    ])\n",
    "    \n",
    "    display(baseline_df.round(4))\n",
    "    \n",
    "    # Best baseline model\n",
    "    if not baseline_df.empty:\n",
    "        best_baseline = baseline_df.loc[baseline_df['Accuracy'].idxmax()]\n",
    "        print(f\"\\nðŸ† BEST BASELINE MODEL: {best_baseline['Model']}\")\n",
    "        print(f\"   ðŸ“Š Baseline Accuracy: {best_baseline['Accuracy']:.4f}\")\n",
    "        print(f\"   ðŸ“Š Baseline F1-Score: {best_baseline['F1-Score']:.4f}\")\n",
    "        print(f\"   ðŸŽ¯ This is our benchmark to beat with fine-tuning!\")\n",
    "        \n",
    "        # Detailed metrics for best baseline\n",
    "        best_results = baseline_results[best_baseline['Model']]\n",
    "        print(f\"\\n   ðŸ“‹ Per-class Performance:\")\n",
    "        classes = ['Negative', 'Neutral', 'Positive']\n",
    "        for i, class_name in enumerate(classes):\n",
    "            if i < len(best_results['precision_per_class']):\n",
    "                precision = best_results['precision_per_class'][i]\n",
    "                recall = best_results['recall_per_class'][i]\n",
    "                f1 = best_results['f1_per_class'][i]\n",
    "                print(f\"      {class_name}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "print(f\"\\nâœ… MODEL SELECTION BASELINE COMPLETED\")\n",
    "print(f\"Successfully evaluated {len(baseline_results)} baseline models\")\n",
    "print(f\"Next step: Fine-tuning selected models for improved performance\")\n",
    "\n",
    "\n",
    "# Clean up GPU memory\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    print(\"ðŸ§¹ GPU memory cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8a26655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE PERFORMANCE (WITHOUT FINE-TUNING) ===\n",
      "This is the performance using pre-trained models directly on our data:\n",
      "\n",
      "BERT (Pre-trained, no fine-tuning):\n",
      "   â€¢ Accuracy: 0.7380 (73.8%)\n",
      "   â€¢ F1-Score: 0.7472\n",
      "\n",
      "RoBERTa (Pre-trained, no fine-tuning):\n",
      "   â€¢ Accuracy: 0.7300 (73.0%)\n",
      "   â€¢ F1-Score: 0.7337\n",
      "\n",
      "DistilBERT (Pre-trained, no fine-tuning):\n",
      "   â€¢ Accuracy: 0.7800 (78.0%)\n",
      "   â€¢ F1-Score: 0.7368\n"
     ]
    }
   ],
   "source": [
    "# Document baseline performance clearly\n",
    "print(\"=== BASELINE PERFORMANCE (WITHOUT FINE-TUNING) ===\")\n",
    "print(\"This is the performance using pre-trained models directly on our data:\")\n",
    "\n",
    "for model_name, results in baseline_results.items():\n",
    "    print(f\"\\n{model_name} (Pre-trained, no fine-tuning):\")\n",
    "    print(f\"   â€¢ Accuracy: {results['accuracy']:.4f} ({results['accuracy']:.1%})\")\n",
    "    print(f\"   â€¢ F1-Score: {results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "72fe9165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3. MODEL EVALUATION ===\n",
      "=== 3.1 EVALUATION METRICS & 3.2 RESULTS ===\n",
      "ðŸ“Š COMPREHENSIVE TRANSFORMER EVALUATION RESULTS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Type</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT</td>\n",
       "      <td>Baseline (Pre-trained)</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.7594</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.7472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>Baseline (Pre-trained)</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.7432</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.7337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DistilBERT</td>\n",
       "      <td>Baseline (Pre-trained)</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.7025</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.7368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model                    Type  Accuracy  Precision  Recall  F1-Score\n",
       "0        BERT  Baseline (Pre-trained)     0.738     0.7594   0.738    0.7472\n",
       "1     RoBERTa  Baseline (Pre-trained)     0.730     0.7432   0.730    0.7337\n",
       "2  DistilBERT  Baseline (Pre-trained)     0.780     0.7025   0.780    0.7368"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† BEST PERFORMING TRANSFORMER MODEL:\n",
      "   ðŸ¥‡ Model: DistilBERT (Baseline (Pre-trained))\n",
      "   ðŸ“Š Accuracy: 0.7800 (78.0%)\n",
      "   ðŸ“Š F1-Score: 0.7368\n",
      "   ðŸ“Š Precision: 0.7025\n",
      "   ðŸ“Š Recall: 0.7800\n",
      "\n",
      "ðŸ“‹ DETAILED RESULTS - DISTILBERT (BASELINE (PRE-TRAINED)):\n",
      "   ðŸŽ¯ Model achieved an accuracy of 78.0% on the validation dataset\n",
      "\n",
      "   ðŸ“Š Per-class Performance:\n",
      "      â€¢ Class Negative:\n",
      "        - Precision: 67.4% (0.6740)\n",
      "        - Recall: 89.7% (0.8971)\n",
      "        - F1-score: 77.0% (0.7697)\n",
      "      â€¢ Class Neutral:\n",
      "        - Precision: 0.0% (0.0000)\n",
      "        - Recall: 0.0% (0.0000)\n",
      "        - F1-score: 0.0% (0.0000)\n",
      "      â€¢ Class Positive:\n",
      "        - Precision: 84.0% (0.8401)\n",
      "        - Recall: 86.7% (0.8673)\n",
      "        - F1-score: 85.4% (0.8535)\n",
      "\n",
      "ðŸ“Š CONFUSION MATRIX - DISTILBERT:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Label  Negative  Neutral  Positive\n",
       "True Label                                  \n",
       "Negative              122        0        14\n",
       "Neutral                18        0        37\n",
       "Positive               41        0       268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ DETAILED CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.90      0.77       136\n",
      "     Neutral       0.00      0.00      0.00        55\n",
      "    Positive       0.84      0.87      0.85       309\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.50      0.59      0.54       500\n",
      "weighted avg       0.70      0.78      0.74       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3. MODEL EVALUATION\n",
    "\n",
    "3.1 Evaluation Metrics - Comprehensive performance evaluation\n",
    "3.2 Results - Detailed results presentation with confusion matrices\n",
    "\n",
    "This cell provides complete evaluation of both baseline and fine-tuned transformer models,\n",
    "comparing performance metrics and analyzing results across different sentiment classes.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== 3. MODEL EVALUATION ===\")\n",
    "print(\"=== 3.1 EVALUATION METRICS & 3.2 RESULTS ===\")\n",
    "\n",
    "all_transformer_results = []\n",
    "if 'baseline_results' not in globals() or baseline_results is None:\n",
    "    baseline_results = {}\n",
    "# Add baseline results (pre-trained models without fine-tuning)\n",
    "for model_name, results in baseline_results.items():\n",
    "    all_transformer_results.append({\n",
    "        'Model': f\"{model_name}\",\n",
    "        'Type': 'Baseline (Pre-trained)',\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1-Score': results['f1'],\n",
    "        'Details': results\n",
    "    })\n",
    "# Add fine-tuned results\n",
    "for model_name, results in fine_tuned_results.items():\n",
    "    all_transformer_results.append({\n",
    "        'Model': f\"{model_name}\",\n",
    "        'Type': 'Fine-tuned',\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1-Score': results['f1'],\n",
    "        'Details': results\n",
    "    })\n",
    "if all_transformer_results:\n",
    "    transformer_comparison_df = pd.DataFrame(all_transformer_results)\n",
    "    print(\"ðŸ“Š COMPREHENSIVE TRANSFORMER EVALUATION RESULTS:\")\n",
    "    display_df = transformer_comparison_df.drop('Details', axis=1)\n",
    "    display(display_df.round(4))\n",
    "    best_model_idx = transformer_comparison_df['Accuracy'].idxmax()\n",
    "    best_model = transformer_comparison_df.loc[best_model_idx]\n",
    "    print(f\"\\nðŸ† BEST PERFORMING TRANSFORMER MODEL:\")\n",
    "    print(f\"   ðŸ¥‡ Model: {best_model['Model']} ({best_model['Type']})\")\n",
    "    print(f\"   ðŸ“Š Accuracy: {best_model['Accuracy']:.4f} ({best_model['Accuracy']:.1%})\")\n",
    "    print(f\"   ðŸ“Š F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "    print(f\"   ðŸ“Š Precision: {best_model['Precision']:.4f}\")\n",
    "    print(f\"   ðŸ“Š Recall: {best_model['Recall']:.4f}\")\n",
    "    best_results = best_model['Details']\n",
    "    print(f\"\\nðŸ“‹ DETAILED RESULTS - {best_model['Model'].upper()} ({best_model['Type'].upper()}):\")\n",
    "    print(f\"   ðŸŽ¯ Model achieved an accuracy of {best_results['accuracy']:.1%} on the validation dataset\")\n",
    "    print(f\"\\n   ðŸ“Š Per-class Performance:\")\n",
    "    classes = ['Negative', 'Neutral', 'Positive']\n",
    "    if 'precision_per_class' in best_results and len(best_results['precision_per_class']) >= 3:\n",
    "        for i, class_name in enumerate(classes):\n",
    "            precision = best_results['precision_per_class'][i]\n",
    "            recall = best_results['recall_per_class'][i]\n",
    "            f1 = best_results['f1_per_class'][i]\n",
    "            print(f\"      â€¢ Class {class_name}:\")\n",
    "            print(f\"        - Precision: {precision:.1%} ({precision:.4f})\")\n",
    "            print(f\"        - Recall: {recall:.1%} ({recall:.4f})\")\n",
    "            print(f\"        - F1-score: {f1:.1%} ({f1:.4f})\")\n",
    "    if 'confusion_matrix' in best_results:\n",
    "        cm = best_results['confusion_matrix']\n",
    "    elif 'true_labels' in best_results and 'predictions' in best_results:\n",
    "        cm = confusion_matrix(\n",
    "            best_results['true_labels'],\n",
    "            best_results['predictions'],\n",
    "            labels=classes\n",
    "        )\n",
    "    else:\n",
    "        cm = np.zeros((3,3))\n",
    "    print(f\"\\nðŸ“Š CONFUSION MATRIX - {best_model['Model'].upper()}:\")\n",
    "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    cm_df.index.name = 'True Label'\n",
    "    cm_df.columns.name = 'Predicted Label'\n",
    "    display(cm_df)\n",
    "    print(f\"\\nðŸ“ˆ DETAILED CLASSIFICATION REPORT:\")\n",
    "    if 'true_labels' in best_results and 'predictions' in best_results:\n",
    "        print(classification_report(\n",
    "            best_results['true_labels'],\n",
    "            best_results['predictions'],\n",
    "            target_names=classes\n",
    "        ))\n",
    "    if best_model['Type'] == 'Fine-tuned':\n",
    "        print(f\"\\nðŸ‹ï¸ TRAINING METRICS:\")\n",
    "        print(f\"   Training Loss: {best_results['training_loss']:.4f}\")\n",
    "        print(f\"   Validation Loss: {best_results['eval_loss']:.4f}\")\n",
    "        if 'training_loss' in best_results and best_results['training_loss']:\n",
    "            loss_ratio = best_results['eval_loss'] / best_results['training_loss']\n",
    "            if loss_ratio < 1.2:\n",
    "                print(f\"   ðŸ“Š Loss Ratio: {loss_ratio:.2f} (Good - No significant overfitting)\")\n",
    "            elif loss_ratio < 1.5:\n",
    "                print(f\"   ðŸ“Š Loss Ratio: {loss_ratio:.2f} (Acceptable - Slight overfitting)\")\n",
    "            else:\n",
    "                print(f\"   ðŸ“Š Loss Ratio: {loss_ratio:.2f} (Warning - Possible overfitting)\")\n",
    "else:\n",
    "    print(\"No transformer results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7e3481a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set class distribution: Counter({'Positive': 6590, 'Negative': 2988, 'Neutral': 1345})\n",
      "XGBoost predictions not available: 'TF-IDF'\n",
      "Random Forest predictions not available: 'TF-IDF'\n"
     ]
    }
   ],
   "source": [
    "# === Diagnostics: Why is Neutral missing? ===\n",
    "from collections import Counter\n",
    "print('Test set class distribution:', Counter(y_test))\n",
    " \n",
    "# Pick two important models to inspect\n",
    "try:\n",
    "    xgb_pred = results['TF-IDF']['XGBoost']['y_pred']\n",
    "    print('XGBoost predictions distribution:', Counter(xgb_pred))\n",
    "except Exception as e:\n",
    "    print('XGBoost predictions not available:', e)\n",
    " \n",
    "try:\n",
    "    rf_pred = results['TF-IDF']['Random Forest']['y_pred']\n",
    "    print('Random Forest predictions distribution:', Counter(rf_pred))\n",
    "except Exception as e:\n",
    "    print('Random Forest predictions not available:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e882d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted class weights (Neutral x1.15): {np.str_('Negative'): np.float64(1.2184388856354054), np.str_('Neutral'): np.float64(5.37213369282261), np.str_('Positive'): np.float64(0.5524828344903456)}\n",
      "\n",
      "Confusion Matrix - RandomForest (Neutral x1.15):\n",
      "\n",
      "Confusion Matrix - RandomForest (Neutral x1.15):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>2256</td>\n",
       "      <td>75</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>14</td>\n",
       "      <td>634</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>442</td>\n",
       "      <td>436</td>\n",
       "      <td>5712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Neutral  Positive\n",
       "Negative      2256       75       657\n",
       "Neutral         14      634       697\n",
       "Positive       442      436      5712"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions distribution (RF): Counter({'Positive': 7066, 'Negative': 2712, 'Neutral': 1145})\n",
      "\n",
      "Confusion Matrix - XGBoost (Neutral x1.15):\n",
      "\n",
      "Confusion Matrix - XGBoost (Neutral x1.15):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>2279</td>\n",
       "      <td>453</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>5</td>\n",
       "      <td>1313</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>772</td>\n",
       "      <td>3789</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Neutral  Positive\n",
       "Negative      2279      453       256\n",
       "Neutral          5     1313        27\n",
       "Positive       772     3789      2029"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions distribution (XGB): Counter({np.str_('Neutral'): 5555, np.str_('Negative'): 3056, np.str_('Positive'): 2312})\n"
     ]
    }
   ],
   "source": [
    "# === XGBoost and Random Forest: dealing with unbalanced in the best and worst models ===\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "# Ensure class_weight_dict exists; if not, compute balanced and then boost Neutral\n",
    "try:\n",
    "    _ = class_weight_dict\n",
    "except NameError:\n",
    "    class_names = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "    cw_vals = compute_class_weight(class_weight='balanced', classes=class_names, y=y_train.values)\n",
    "    class_weight_dict = {cls: w for cls, w in zip(class_names, cw_vals)}\n",
    " \n",
    "# Boost Neutral by 1.15x\n",
    "class_weight_dict['Neutral'] = class_weight_dict.get('Neutral', 1.0) * 1.15\n",
    "print('Adjusted class weights (Neutral x1.15):', class_weight_dict)\n",
    " \n",
    "# ---- RandomForest with boosted Neutral weight ----\n",
    "rf_115 = RandomForestClassifier(\n",
    "    random_state=42, n_estimators=400, max_depth=None, n_jobs=-1,\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "rf_115.fit(X_train_tfidf_bal, y_train_tfidf_bal)\n",
    "rf_115_pred = rf_115.predict(X_test_tfidf)\n",
    "classes = ['Negative', 'Neutral', 'Positive']\n",
    "cm_rf_115 = confusion_matrix(y_test, rf_115_pred, labels=classes)\n",
    "cm_rf_115_df = pd.DataFrame(cm_rf_115, index=classes, columns=classes)\n",
    "print('\\nConfusion Matrix - RandomForest (Neutral x1.15):')\n",
    "display(cm_rf_115_df)\n",
    "print('Predictions distribution (RF):', Counter(rf_115_pred))\n",
    " \n",
    "# ---- XGBoost with boosted Neutral weight ----\n",
    "# Build sample weights reflecting boosted Neutral\n",
    "xgb_sw_tfidf_115 = np.array([class_weight_dict[y] for y in y_train_tfidf_bal])\n",
    "le_tmp = LabelEncoder().fit(['Negative','Neutral','Positive'])\n",
    "y_train_enc = le_tmp.transform(y_train_tfidf_bal)\n",
    "xgb_115 = XGBClassifier(random_state=42, n_estimators=200, max_depth=6, learning_rate=0.1, eval_metric='mlogloss', verbosity=0)\n",
    "xgb_115.fit(X_train_tfidf_bal, y_train_enc, sample_weight=xgb_sw_tfidf_115)\n",
    "y_pred_enc = xgb_115.predict(X_test_tfidf)\n",
    "xgb_115_pred = le_tmp.inverse_transform(y_pred_enc)\n",
    "cm_xgb_115 = confusion_matrix(y_test, xgb_115_pred, labels=classes)\n",
    "cm_xgb_115_df = pd.DataFrame(cm_xgb_115, index=classes, columns=classes)\n",
    "print('\\nConfusion Matrix - XGBoost (Neutral x1.15):')\n",
    "display(cm_xgb_115_df)\n",
    "print('Predictions distribution (XGB):', Counter(xgb_115_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b4d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to build pipeline for ELECTRA: google/electra-base-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline created OK. Run a small sample:\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: try to build the sentiment pipeline for ELECTRA and show full exception\n",
    "import traceback\n",
    "from transformers import pipeline\n",
    "try:\n",
    "    model_id = baseline_sentiment_models.get('ELECTRA', 'google/electra-base-discriminator')\n",
    "    print(\"Attempting to build pipeline for ELECTRA:\", model_id)\n",
    "    clf = pipeline(\"sentiment-analysis\", model=model_id, tokenizer=model_id, device=(0 if torch.cuda.is_available() else -1))\n",
    "    print(\"Pipeline created OK. Run a small sample:\")\n",
    "    print(clf(\"This product is fine, nothing special\"))\n",
    "except Exception as e:\n",
    "    print(\"Pipeline creation or sample inference failed for ELECTRA. Full traceback below:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e0346",
   "metadata": {},
   "source": [
    "Positive stayed the dominant class after your sampling strategy (you upsampled Neutral to match Negative but left Positive at its original, larger size) â†’ model still biased toward Positive.\n",
    "You trained the RandomForest on the ROS-balanced data but also passed a custom class_weight dict computed from the original y_train (or used both oversampling + class_weight) â€” that mismatch / double-correction can push decisions toward the majority class.\n",
    "Neutral vs Positive are often semantically close (rating 4â†’Neutral vs 5â†’Positive); TFâ€‘IDF features may not separate them well so RF prefers the stronger Positive signal.\n",
    "Default decision rule (argmax on votes/probs) + uncalibrated probabilities â†’ low-probability Neutral gets mapped to Positive.\n",
    "\n",
    "\n",
    "Recommendation: run the diagnostic cell first to confirm which cause is dominant (Positive still majority? avg Positive prob on true-Neutral high?). Then either (A) stop double-correcting (remove class_weight when training on ROS-balanced data) or (B) recompute class_weight from balanced training and/or tune threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2ee4e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain original dist:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Counter(\u001b[43my_train\u001b[49m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain after ROS (tfidf) dist:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Counter(y_train_tfidf_bal))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest dist:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Counter(y_test))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Diagnostic: why many Positive false-positives (run this cell)\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import inspect\n",
    "\n",
    "print(\"Train original dist:\", Counter(y_train))\n",
    "print(\"Train after ROS (tfidf) dist:\", Counter(y_train_tfidf_bal))\n",
    "print(\"Test dist:\", Counter(y_test))\n",
    "print()\n",
    "\n",
    "# show class_weight_dict if present\n",
    "try:\n",
    "    print(\"class_weight_dict:\", class_weight_dict)\n",
    "except Exception:\n",
    "    print(\"class_weight_dict not defined\")\n",
    "\n",
    "# find an RF model to inspect\n",
    "rf_candidates = ['rf_115', 'best_rf', 'trained_models_tfidf', 'trained_models_count']\n",
    "rf_model = None\n",
    "if 'rf_115' in globals():\n",
    "    rf_model = globals()['rf_115']; print(\"Using rf_115\")\n",
    "elif 'best_rf' in globals():\n",
    "    rf_model = globals()['best_rf']; print(\"Using best_rf\")\n",
    "elif 'trained_models_tfidf' in globals() and 'Random Forest' in trained_models_tfidf:\n",
    "    rf_model = trained_models_tfidf['Random Forest']; print(\"Using trained_models_tfidf['Random Forest']\")\n",
    "elif 'trained_models_count' in globals() and 'Random Forest' in trained_models_count:\n",
    "    rf_model = trained_models_count['Random Forest']; print(\"Using trained_models_count['Random Forest']\")\n",
    "else:\n",
    "    # search globals for RandomForestClassifier instance\n",
    "    for name, obj in globals().items():\n",
    "        try:\n",
    "            if isinstance(obj, (type(np))): pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"No RF candidate found automatically. Define rf_115 or best_rf or trained_models_tfidf['Random Forest'].\")\n",
    "\n",
    "if rf_model is not None:\n",
    "    # predictions & confusion\n",
    "    try:\n",
    "        y_pred = rf_model.predict(X_test_tfidf)\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        classes = ['Negative','Neutral','Positive']\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "        print(\"\\nConfusion matrix (RF):\")\n",
    "        display(pd.DataFrame(cm, index=classes, columns=classes))\n",
    "        print(\"Prediction dist:\", Counter(y_pred))\n",
    "    except Exception as e:\n",
    "        print(\"RF predict failed:\", e)\n",
    "\n",
    "    # probabilities\n",
    "    if hasattr(rf_model, \"predict_proba\"):\n",
    "        probs = rf_model.predict_proba(X_test_tfidf)\n",
    "        print(\"\\nRF.classes_:\", list(rf_model.classes_))\n",
    "        mean_probs = dict(zip(rf_model.classes_, np.round(probs.mean(axis=0),4)))\n",
    "        print(\"Mean predicted probability per class (test):\", mean_probs)\n",
    "\n",
    "        if 'Neutral' in rf_model.classes_:\n",
    "            idx_neu = list(rf_model.classes_).index('Neutral')\n",
    "            # avg Neutral prob for true-Neutral rows\n",
    "            y_test_arr = np.asarray(y_test)\n",
    "            true_neu_idx = np.where(y_test_arr == 'Neutral')[0]\n",
    "            print(\"True Neutral count (test):\", len(true_neu_idx))\n",
    "            if len(true_neu_idx)>0:\n",
    "                avg_neu_prob_on_true = probs[true_neu_idx, idx_neu].mean()\n",
    "                avg_pos_prob_on_true_neu = probs[true_neu_idx, list(rf_model.classes_).index('Positive')].mean()\n",
    "                print(f\"Avg RF Neutral prob on true-Neutral rows: {avg_neu_prob_on_true:.4f}\")\n",
    "                print(f\"Avg RF Positive prob on true-Neutral rows: {avg_pos_prob_on_true_neu:.4f}\")\n",
    "                # show few worst true-Neutral rows by Neutral prob\n",
    "                worst = np.argsort(probs[true_neu_idx, idx_neu])[:5]\n",
    "                print(\"\\nTop 5 true-Neutral rows with lowest Neutral prob (global_index, pred, neu_prob, pos_prob):\")\n",
    "                for r in worst:\n",
    "                    gi = true_neu_idx[r]\n",
    "                    print(gi, y_pred[gi], probs[gi, idx_neu].round(4), probs[gi, list(rf_model.classes_).index('Positive')].round(4))\n",
    "    else:\n",
    "        print(\"RF has no predict_proba\")\n",
    "\n",
    "    # feature importances (map to TF-IDF features if available)\n",
    "    try:\n",
    "        fi = rf_model.feature_importances_\n",
    "        feat_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        top_idx = np.argsort(fi)[-20:][::-1]\n",
    "        print(\"\\nTop RF feature importances (top 20):\")\n",
    "        for ix in top_idx:\n",
    "            print(f\"{feat_names[ix]}: {fi[ix]:.5f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Feature importances unavailable or mapping failed:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
